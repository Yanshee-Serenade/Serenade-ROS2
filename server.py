from flask import Flask, request, Response, jsonify
from flask_cors import CORS
from PIL import Image
from io import BytesIO
import torch
import json
import datetime
import os
import time
from threading import Thread
import numpy as np
import cv2
import matplotlib
matplotlib.use('Agg')  # éäº¤äº’å¼åç«¯ï¼Œæ”¯æŒæ— æ¡Œé¢ç¯å¢ƒä¿å­˜å›¾ç‰‡
import matplotlib.pyplot as plt
from transformers import AutoProcessor, AutoModelForImageTextToText
from transformers import TextIteratorStreamer
from depth_anything_3.api import DepthAnything3
from depth_anything_3.utils.visualize import visualize_depth
from sam3.model_builder import build_sam3_image_model
from sam3.model.sam3_image_processor import Sam3Processor
from sam3.visualization_utils import plot_results

# å¯¼å…¥é‡æ„åçš„ros_apiï¼ˆç¡®ä¿åŒ…å«TrackingDataClientå’ŒTrackingResultç­‰ç±»å‹ï¼‰
import ros_api
from ros_api import TrackingResult, CameraIntrinsics, CameraPose  # å¯¼å…¥å¼ºç±»å‹æ•°æ®ç»“æ„

# ===================== é…ç½®å¸¸é‡ =====================
# æ¨¡å‹é…ç½®
MODEL_QWEN_8B = "Qwen/Qwen3-VL-8B-Instruct"
MODEL_QWEN_4B = "Qwen/Qwen3-VL-4B-Instruct"
MODEL_QWEN_2B = "Qwen/Qwen3-VL-2B-Instruct"
MODEL_SMOLVLM = "HuggingFaceTB/SmolVLM2-2.2B-Instruct"
MODEL_DA3_LARGE = "depth-anything/DA3-LARGE-1.1"
MODEL_DA3_NESTED = "depth-anything/DA3NESTED-GIANT-LARGE-1.1"
MODEL_VLM_DEFAULT = MODEL_QWEN_4B
MODEL_DA3_DEFAULT = MODEL_DA3_LARGE
MODEL_SAM3_PATH = "/home/seqn/sam3/sam3.pt"  # æ ¹æ®è‡ªå·±çš„æ¨¡å‹æƒé‡åœ°å€ä¿®æ”¹

# ç½‘ç»œé…ç½®
ROS_SERVER_IP = "127.0.0.1"
ROS_SERVER_PORT = 51121
FLASK_HOST = "0.0.0.0"
FLASK_PORT = 51122

# ç”Ÿæˆé…ç½®
MAX_NEW_TOKENS = 256
IMAGE_SAVE_PREFIX = "images/image_"
DEPTH_PLOT_SAVE_PREFIX = "images/depth_comparison_"
DA3_DEPTH_SAVE_PREFIX = "images/da3_depth_"
DA3_DEPTH_WITH_KEYPOINTS_SAVE_PREFIX = "images/da3_depth_with_keypoints_"

# ç¡®ä¿å›¾åƒä¿å­˜ç›®å½•å­˜åœ¨
os.makedirs("images", exist_ok=True)

# ===================== å…¨å±€å¯¹è±¡ =====================
app = None
processor = None
model_vlm = None
model_da3 = None
processor_sam3 = None

def init_tracking_client(enable_log: bool = False) -> ros_api.TrackingDataClient:
    """
    æ–°å»ºå¹¶è¿”å›ROSè·Ÿè¸ªæ•°æ®å®¢æˆ·ç«¯å®ä¾‹ï¼ˆæ¯æ¬¡è°ƒç”¨æ–°å»ºè¿æ¥ï¼Œé€‚é…é‡æ„ç‰ˆå®¢æˆ·ç«¯ï¼‰
    :param enable_log: æ˜¯å¦å¯ç”¨å®¢æˆ·ç«¯æ—¥å¿—ï¼ˆé»˜è®¤å…³é—­ï¼Œé¿å…ä¸Flaskæ—¥å¿—å†²çªï¼‰
    :return: TrackingDataClientå®ä¾‹ï¼ˆæ— éœ€æå‰è¿æ¥ï¼Œé—­ç¯æ–¹æ³•å†…éƒ¨å¤„ç†è¿æ¥ï¼‰
    """
    try:
        # å®ä¾‹åŒ–é‡æ„ç‰ˆros_apiå®¢æˆ·ç«¯ï¼ˆä»…åˆå§‹åŒ–ï¼Œä¸æå‰è¿æ¥ï¼‰
        client = ros_api.TrackingDataClient(
            server_ip=ROS_SERVER_IP,
            port=ROS_SERVER_PORT,
            enable_log=enable_log  # å…³é—­å®¢æˆ·ç«¯æ—¥å¿—ï¼Œç”±Flaskç»Ÿä¸€è¾“å‡º
        )
        return client
    except Exception as e:
        error_time = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')
        print(f"[{error_time}] âŒ è·Ÿè¸ªå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")
        raise Exception(f"è·Ÿè¸ªå®¢æˆ·ç«¯åˆ›å»ºå¤±è´¥: {str(e)}")

def get_image_from_ros(client: ros_api.TrackingDataClient, timestamp: str) -> tuple[Image.Image, str, np.ndarray, np.ndarray, np.ndarray] | tuple[None, str, None, None, None]:
    """
    ä»ä¼ å…¥çš„ROSå®¢æˆ·ç«¯è·å–å›¾åƒã€ç‚¹äº‘æ•°æ®å¹¶è½¬æ¢ä¸ºPIL Imageï¼ˆé€‚é…é—­ç¯æ–¹æ³•+å¼ºç±»å‹è¿”å›å€¼ï¼‰
    :param client: é‡æ„ç‰ˆTrackingDataClientå®ä¾‹
    :param timestamp: æ—¶é—´æˆ³ï¼ˆç”¨äºç”Ÿæˆå›¾åƒæ–‡ä»¶åï¼‰
    :return: (PILå›¾åƒå¯¹è±¡, å›¾åƒä¿å­˜è·¯å¾„, ç›¸æœºåæ ‡ç‚¹äº‘, ä¸–ç•Œåæ ‡ç‚¹äº‘, åŸå§‹OpenCVå›¾åƒ) æˆ–é”™è¯¯å…ƒç»„
    """
    if not client:
        return None, "ROSå®¢æˆ·ç«¯å®ä¾‹æ— æ•ˆ", None, None, None
    
    try:
        # ============== æ ¸å¿ƒé‡æ„ï¼šè°ƒç”¨é—­ç¯æ–¹æ³•ä¸€é”®å®Œæˆå…¨æµç¨‹ ==============
        print(f"[{timestamp}] ğŸ” å¼€å§‹æ‰§è¡ŒROSæ•°æ®é—­ç¯è·å–æµç¨‹...")
        tracking_result: TrackingResult | None = client.complete_tracking_pipeline()
        
        # æ ¡éªŒé—­ç¯æ–¹æ³•è¿”å›ç»“æœï¼ˆå¼ºç±»å‹å¯¹è±¡ï¼‰
        if not tracking_result:
            return None, "ROSæ•°æ®é—­ç¯è·å–å¤±è´¥ï¼ˆè¿æ¥/è§£æ/è¯·æ±‚ä»»ä¸€ç¯èŠ‚å‡ºé”™ï¼‰", None, None, None
        
        # ============== ä»å¼ºç±»å‹TrackingResultä¸­æå–æ•°æ®ï¼ˆæ›¿æ¢åŸå­—å…¸å–å€¼ï¼‰ ==============
        # 1. æå–OpenCVå›¾åƒï¼ˆä¿ç•™åŸå§‹å›¾åƒï¼Œç”¨äºåç»­åŒ¹é…æ·±åº¦å›¾å°ºå¯¸ï¼‰
        cv_image = tracking_result.current_image
        if cv_image is None or not isinstance(cv_image, np.ndarray):
            return None, "ä»ROSé—­ç¯ç»“æœä¸­æå–å›¾åƒå¤±è´¥", None, None, None
        
        # 2. æå–ORB-SLAM3ç‚¹äº‘æ•°æ®ï¼ˆç›¸æœºåæ ‡/ä¸–ç•Œåæ ‡ï¼‰
        camera_point_cloud = tracking_result.tracked_points_camera
        world_point_cloud = tracking_result.tracked_points_world
        
        # 3. OpenCVå›¾åƒè½¬æ¢ä¸ºPIL Imageï¼ˆCV2: BGR â†’ PIL: RGBï¼‰
        cv_image_rgb = cv2.cvtColor(cv_image, cv2.COLOR_BGR2RGB)
        pil_image = Image.fromarray(cv_image_rgb)
        
        # 4. ä¿å­˜å›¾åƒåˆ°æœ¬åœ°
        image_save_path = f"{IMAGE_SAVE_PREFIX}{timestamp}.jpg"
        pil_image.save(image_save_path)
        print(f"[{timestamp}] ğŸ’¾ ä¿å­˜å¤„ç†åå›¾åƒåˆ°: {image_save_path}ï¼ˆå°ºå¯¸ï¼š{pil_image.size}ï¼Œæ¨¡å¼ï¼š{pil_image.mode}ï¼‰")
        print(f"[{timestamp}] ğŸ“Š ROSæ•°æ®é—­ç¯è·å–ç»Ÿè®¡ï¼šæ€»æ¥æ”¶{tracking_result.total_recv_size}å­—èŠ‚ï¼Œè§£æè€—æ—¶{tracking_result.parse_cost_ms:.2f}ms")
        
        return pil_image, image_save_path, camera_point_cloud, world_point_cloud, cv_image
    
    except Exception as e:
        error_msg = f"ä»ROSé—­ç¯ç»“æœå¤„ç†æ•°æ®å¤±è´¥: {str(e)}"
        print(f"[{timestamp}] âŒ {error_msg}")
        return None, error_msg, None, None, None

# ===================== æ¨¡å‹åŠ è½½æ¨¡å—ï¼ˆæ— ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰ =====================
def load_model_vlm(model_path: str = MODEL_VLM_DEFAULT):
    """
    åŠ è½½å¹¶ç¼–è¯‘AIæ¨¡å‹
    :param model_path: æ¨¡å‹è·¯å¾„/åç§°
    """
    global processor, model_vlm
    try:
        # 1. åŠ è½½å¤„ç†å™¨
        print(f"{time.time()} > åŠ è½½æ¨¡å‹å¤„ç†å™¨: {model_path}...", flush=True)
        processor = AutoProcessor.from_pretrained(model_path)
        
        # 2. åŠ è½½æ¨¡å‹
        print(f"{time.time()} > åŠ è½½æ¨¡å‹æƒé‡...", flush=True)
        model_vlm = AutoModelForImageTextToText.from_pretrained(
            model_path,
            load_in_8bit=True
        )
        
        # 3. ç¼–è¯‘æ¨¡å‹ï¼ˆä¼˜åŒ–æ¨ç†é€Ÿåº¦ï¼‰
        print(f"{time.time()} > ç¼–è¯‘æ¨¡å‹...", flush=True)
        model_vlm = torch.compile(model_vlm)
        
        print(f"{time.time()} > âœ… VLM æ¨¡å‹åŠ è½½å¹¶ç¼–è¯‘å®Œæˆï¼", flush=True)
    except Exception as e:
        raise Exception(f"æ¨¡å‹åŠ è½½å¤±è´¥: {str(e)}")

def load_model_da3(model_path=MODEL_DA3_DEFAULT):
    """åŠ è½½DA3æ·±åº¦ä¼°è®¡æ¨¡å‹å¹¶åˆå§‹åŒ–å…¨å±€å¯¹è±¡"""
    global model_da3
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model_da3 = DepthAnything3.from_pretrained(model_path)
    model_da3 = model_da3.to(device)
    model_da3.eval()
    print(f"{time.time()} > âœ… DA3 æ¨¡å‹åŠ è½½å¹¶ç¼–è¯‘å®Œæˆï¼", flush=True)

def load_model_sam3(model_path=MODEL_SAM3_PATH):
    """åŠ è½½SAM3æ¨¡å‹å¹¶åˆå§‹åŒ–å…¨å±€å¯¹è±¡"""
    global processor_sam3
    model = build_sam3_image_model(
        load_from_HF=False,
        checkpoint_path=model_path
    )
    processor_sam3 = Sam3Processor(model)
    print(f"{time.time()} > âœ… SAM3 æ¨¡å‹åŠ è½½å¹¶ç¼–è¯‘å®Œæˆï¼", flush=True)

# ===================== æ·±åº¦ç”Ÿæˆæ¨¡å—ï¼ˆæ— ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰ =====================
def generate_depth_map(image_path: str, target_shape: tuple[int, int]):
    """
    ç”ŸæˆæŒ‡å®šå°ºå¯¸çš„æ·±åº¦å›¾ï¼Œä½¿ç”¨INTER_CUBICæ’å€¼è¿›è¡Œç¼©æ”¾ï¼ŒåŒ¹é…åŸå§‹å›¾åƒå°ºå¯¸
    :param image_path: è¾“å…¥å›¾åƒè·¯å¾„
    :param target_shape: ç›®æ ‡æ·±åº¦å›¾å°ºå¯¸ (h, w)ï¼ˆä¸åŸå§‹å›¾åƒä¸€è‡´ï¼‰
    :return: ä¸ç›®æ ‡å°ºå¯¸åŒ¹é…çš„æ·±åº¦å›¾æ•°ç»„
    """
    global model_da3
    if model_da3 is None:
        raise Exception("DA3æ¨¡å‹æœªåˆå§‹åŒ–ï¼Œè¯·å…ˆè°ƒç”¨load_model_da3()")
    
    prediction = model_da3.inference(
        image=[image_path],
        process_res=504,
        process_res_method="upper_bound_resize",
        export_dir=None,
        export_format="glb"
    )
    
    # æå–é¢„æµ‹æ·±åº¦å›¾å¹¶è°ƒæ•´å°ºå¯¸è‡³ç›®æ ‡å½¢çŠ¶ï¼Œä½¿ç”¨INTER_CUBICæ’å€¼ä¿è¯ç²¾åº¦
    depth_map = prediction.depth[0]
    depth_map_resized = cv2.resize(
        depth_map,
        (target_shape[1], target_shape[0]),  # cv2.resizeå‚æ•°ä¸º(w, h)ï¼Œä¸target_shape (h, w)å¯¹åº”
        interpolation=cv2.INTER_CUBIC
    )
    
    return depth_map_resized

# ===================== æ·±åº¦å¯¹æ¯”ç»˜å›¾æ¨¡å—ï¼ˆæ— ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰ =====================
def plot_depth_comparison(camera_point_cloud: np.ndarray, da3_depth_map: np.ndarray, timestamp: str, image_shape: tuple[int, int]):
    """
    ä¸¥æ ¼æ ¹æ®camera_point_cloudçš„x/yï¼ˆåƒç´ åæ ‡ï¼‰æå–å¯¹åº”DA3æ·±åº¦ï¼Œzè½´ä¸ºå®é™…æ·±åº¦ï¼Œç»˜åˆ¶å…³ç³»å›¾å¹¶ä¿å­˜
    :param camera_point_cloud: ç›¸æœºåæ ‡ç‚¹äº‘ (N, 3)ï¼Œx=åƒç´ w, y=åƒç´ h, z=ORB-SLAM3å®é™…æ·±åº¦
    :param da3_depth_map: DA3ç”Ÿæˆçš„æ·±åº¦å›¾ (h, w)
    :param timestamp: æ—¶é—´æˆ³ï¼Œç”¨äºç”Ÿæˆä¿å­˜æ–‡ä»¶å
    :param image_shape: åŸå§‹å›¾åƒå°ºå¯¸ (h, w)ï¼Œç”¨äºæ ¡éªŒåƒç´ åæ ‡æœ‰æ•ˆæ€§
    """
    # 1. å‰ç½®æ ¡éªŒ
    if camera_point_cloud is None or len(camera_point_cloud) == 0:
        print(f"[{timestamp}] âš ï¸  æ— æœ‰æ•ˆORB-SLAM3ç‚¹äº‘æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
        return
    if da3_depth_map.shape != image_shape:
        print(f"[{timestamp}] âš ï¸  æ·±åº¦å›¾å°ºå¯¸ä¸å›¾åƒå°ºå¯¸ä¸åŒ¹é…ï¼Œè·³è¿‡ç»˜å›¾")
        return
    
    # 2. æå–ç‚¹äº‘æ•°æ®ï¼Œåˆ†ç¦»xï¼ˆåƒç´ w/åˆ—ï¼‰ã€yï¼ˆåƒç´ h/è¡Œï¼‰ã€zï¼ˆå®é™…æ·±åº¦ï¼‰
    pixel_w = camera_point_cloud[:, 0].astype(np.int32)  # xå¯¹åº”å›¾åƒåˆ—ï¼ˆwï¼‰
    pixel_h = camera_point_cloud[:, 1].astype(np.int32)  # yå¯¹åº”å›¾åƒè¡Œï¼ˆhï¼‰
    orb_slam_depth = camera_point_cloud[:, 2]  # zè½´ä¸ºORB-SLAM3å®é™…æ·±åº¦ï¼ˆçœŸå®å€¼ï¼‰
    
    # 3. è¿‡æ»¤æ— æ•ˆæ•°æ®
    valid_mask = np.logical_and.reduce([
        orb_slam_depth > 0,  # è¿‡æ»¤æ— æ•ˆæ·±åº¦ï¼ˆ<=0ï¼‰
        pixel_w >= 0,
        pixel_w < image_shape[1],  # è¿‡æ»¤è¶…å‡ºå›¾åƒå®½åº¦çš„åƒç´ åæ ‡
        pixel_h >= 0,
        pixel_h < image_shape[0]   # è¿‡æ»¤è¶…å‡ºå›¾åƒé«˜åº¦çš„åƒç´ åæ ‡
    ])
    
    # 4. æå–æœ‰æ•ˆæ•°æ®
    valid_pixel_w = pixel_w[valid_mask]
    valid_pixel_h = pixel_h[valid_mask]
    valid_orb_slam_depth = orb_slam_depth[valid_mask]
    
    if len(valid_orb_slam_depth) == 0:
        print(f"[{timestamp}] âš ï¸  æ— æœ‰æ•ˆåƒç´ åæ ‡æˆ–æ·±åº¦æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
        return
    
    # 5. ä¸¥æ ¼æ ¹æ®æœ‰æ•ˆåƒç´ åæ ‡ï¼ˆh, wï¼‰æå–DA3å¯¹åº”ä½ç½®çš„æ·±åº¦å€¼
    # da3_depth_map[pixel_h, pixel_w] å¯¹åº”ï¼šè¡Œ=pixel_hï¼Œåˆ—=pixel_wï¼Œä¸å›¾åƒåæ ‡ä¸€ä¸€å¯¹åº”
    valid_da3_depth = da3_depth_map[valid_pixel_h, valid_pixel_w]
    
    # 6. å†æ¬¡è¿‡æ»¤DA3æ— æ•ˆæ·±åº¦ï¼ˆ<=0ï¼‰
    final_valid_mask = valid_da3_depth > 0
    final_orb_slam_depth = valid_orb_slam_depth[final_valid_mask]
    final_da3_depth = valid_da3_depth[final_valid_mask]
    
    if len(final_orb_slam_depth) == 0:
        print(f"[{timestamp}] âš ï¸  æ— æœ‰æ•ˆDA3æ·±åº¦æ•°æ®ï¼Œè·³è¿‡ç»˜å›¾")
        return
    
    # 7. ç»˜åˆ¶å…³ç³»å›¾ï¼ˆä¿ç•™ä¸€ä¸€å¯¹åº”å…³ç³»ï¼Œä¸ä¸¢å¤±ç‚¹äº‘ä¿¡æ¯ï¼‰
    plt.figure(figsize=(12, 6))
    
    # å­å›¾1ï¼šæ•£ç‚¹å›¾ï¼ˆå±•ç¤ºä¸¤è€…ä¸€ä¸€å¯¹åº”çš„æ·±åº¦å…³ç³»ï¼Œæ ¸å¿ƒå¯¹æ¯”ï¼‰
    plt.subplot(1, 2, 1)
    plt.scatter(final_orb_slam_depth, final_da3_depth, alpha=0.7, s=8, c='royalblue')
    # æ·»åŠ å¯¹è§’çº¿ï¼ˆç†æƒ³æƒ…å†µä¸‹ï¼Œä¸¤è€…æ·±åº¦åº”è½åœ¨å¯¹è§’çº¿ä¸Šï¼‰
    max_depth = np.max([np.max(final_orb_slam_depth), np.max(final_da3_depth)])
    plt.plot([0, max_depth], [0, max_depth], 'r--', alpha=0.8, label="Ideal Match")
    plt.xlabel("ORB-SLAM3 True Depth (m)")
    plt.ylabel("DA3 Predicted Depth (m)")
    plt.title("ORB-SLAM3 vs DA3 Depth (Pixel-wise Match)")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # å­å›¾2ï¼šå·®å€¼ç›´æ–¹å›¾ï¼ˆå±•ç¤ºä¸¤è€…æ·±åº¦è¯¯å·®åˆ†å¸ƒï¼‰
    plt.subplot(1, 2, 2)
    depth_diff = final_orb_slam_depth - final_da3_depth
    plt.hist(depth_diff, bins=50, alpha=0.7, color='purple', edgecolor='black', linewidth=0.5)
    plt.axvline(x=0, color='red', linestyle='--', alpha=0.8, label="Zero Error")
    plt.xlabel("Depth Error (ORB-SLAM3 - DA3) (m)")
    plt.ylabel("Point Count")
    plt.title("Depth Error Distribution Histogram")
    plt.legend()
    plt.grid(True, alpha=0.3)
    
    # 8. è°ƒæ•´å¸ƒå±€å¹¶ä¿å­˜å›¾ç‰‡
    plt.tight_layout()
    plot_save_path = f"{DEPTH_PLOT_SAVE_PREFIX}{timestamp}.png"
    plt.savefig(plot_save_path, dpi=300, bbox_inches='tight')
    plt.close()
    
    # 9. æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print(f"[{timestamp}] ğŸ’¾ æ·±åº¦å¯¹æ¯”å›¾ä¿å­˜åˆ°: {plot_save_path}")
    print(f"[{timestamp}] ğŸ“Š æœ‰æ•ˆå¯¹æ¯”ç‚¹æ•°é‡: {len(final_orb_slam_depth)}")
    print(f"[{timestamp}] ğŸ“Š å¹³å‡æ·±åº¦è¯¯å·®: {np.mean(np.abs(depth_diff)):.6f} m")
    print(f"[{timestamp}] ğŸ“Š å‡æ–¹æ ¹è¯¯å·®: {np.sqrt(np.mean(depth_diff ** 2)):.6f} m")

def save_da3_depth_with_ros_keypoints(da3_depth_map: np.ndarray, camera_point_cloud: np.ndarray, timestamp: str, image_shape: tuple[int, int]):
    """
    1. ä¿å­˜åŸå§‹DA3æ·±åº¦å›¾ï¼ˆç€è‰²å¯è§†åŒ–ï¼‰
    2. åœ¨æ·±åº¦å›¾ä¸Šå åŠ ROSå…³é”®ç‚¹ï¼šç™½è‰²è¾¹æ¡† + ORB-SLAM3çœŸå®æ·±åº¦è‰²å¡«å……ï¼Œä¾¿äºç›´è§‚åˆ¤æ–­æ·±åº¦è¯¯å·®
    :param da3_depth_map: DA3ç”Ÿæˆçš„æ·±åº¦å›¾ (h, w)
    :param camera_point_cloud: ç›¸æœºåæ ‡ç‚¹äº‘ (N, 3)ï¼Œx=åƒç´ w, y=åƒç´ h, z=ORB-SLAM3å®é™…æ·±åº¦
    :param timestamp: æ—¶é—´æˆ³ï¼Œç”¨äºç”Ÿæˆä¿å­˜æ–‡ä»¶å
    :param image_shape: åŸå§‹å›¾åƒå°ºå¯¸ (h, w)ï¼Œç”¨äºæ ¡éªŒåƒç´ åæ ‡æœ‰æ•ˆæ€§
    """
    # å‰ç½®æ ¡éªŒ
    if da3_depth_map is None or da3_depth_map.shape != image_shape:
        print(f"[{timestamp}] âš ï¸  DA3æ·±åº¦å›¾æ— æ•ˆæˆ–å°ºå¯¸ä¸åŒ¹é…ï¼Œè·³è¿‡æ·±åº¦å›¾ä¿å­˜å’Œå…³é”®ç‚¹å åŠ ")
        return
    
    # æ­¥éª¤1ï¼šå¯è§†åŒ–DA3æ·±åº¦å›¾ï¼ˆç€è‰²ï¼Œä¸depth_anything_3é£æ ¼ä¸€è‡´ï¼‰
    da3_depth_viz = visualize_depth(da3_depth_map, cmap='plasma')  # plasmaç€è‰²æ–¹æ¡ˆ
    da3_depth_viz = (da3_depth_viz * 255).astype(np.uint8)  # å½’ä¸€åŒ–å€¼è½¬255çº§RGB
    if len(da3_depth_viz.shape) == 2:  # ç°åº¦å›¾è½¬RGB
        da3_depth_viz = cv2.cvtColor(da3_depth_viz, cv2.COLOR_GRAY2RGB)
    
    # æ­¥éª¤2ï¼šä¿å­˜åŸå§‹ç€è‰²DA3æ·±åº¦å›¾
    da3_depth_save_path = f"{DA3_DEPTH_SAVE_PREFIX}{timestamp}.png"
    cv2.imwrite(da3_depth_save_path, da3_depth_viz)
    print(f"[{timestamp}] ğŸ’¾ åŸå§‹DA3ç€è‰²æ·±åº¦å›¾ä¿å­˜åˆ°: {da3_depth_save_path}")
    
    # æ­¥éª¤3ï¼šå åŠ ROSå…³é”®ç‚¹ï¼ˆç™½è‰²è¾¹æ¡† + ORB-SLAM3çœŸå®æ·±åº¦è‰²å¡«å……ï¼‰
    da3_depth_with_keypoints = da3_depth_viz.copy()
    if camera_point_cloud is not None and len(camera_point_cloud) > 0:
        # æå–ç‚¹äº‘åƒç´ åæ ‡å’ŒORB-SLAM3çœŸå®æ·±åº¦å€¼
        pixel_w = camera_point_cloud[:, 0].astype(np.int32)
        pixel_h = camera_point_cloud[:, 1].astype(np.int32)
        orb_slam_depth = camera_point_cloud[:, 2]  # æå–ORB-SLAM3çœŸå®æ·±åº¦ï¼ˆæ ¸å¿ƒï¼šä»è¿™é‡Œè·å–é¢œè‰²æ˜ å°„ä¾æ®ï¼‰
        
        # è¿‡æ»¤æœ‰æ•ˆåƒç´ åæ ‡å’Œæœ‰æ•ˆæ·±åº¦å€¼
        valid_mask = np.logical_and.reduce([
            orb_slam_depth > 0,
            pixel_w >= 0,
            pixel_w < image_shape[1],
            pixel_h >= 0,
            pixel_h < image_shape[0]
        ])
        
        valid_pixel_w = pixel_w[valid_mask]
        valid_pixel_h = pixel_h[valid_mask]
        valid_orb_slam_depth = orb_slam_depth[valid_mask]  # æœ‰æ•ˆORB-SLAM3çœŸå®æ·±åº¦å€¼
        
        if len(valid_pixel_w) > 0:
            # å…³é”®ç‚¹å‚æ•°ï¼šå¤–åœ†ï¼ˆç™½è‰²è¾¹æ¡†ï¼‰åŠå¾„3ï¼Œå†…åœ†ï¼ˆORBçœŸå®æ·±åº¦è‰²ï¼‰åŠå¾„2
            outer_radius = 3
            inner_radius = 2
            white_color = (255, 255, 255)  # ç™½è‰²è¾¹æ¡†ï¼ˆBGRæ ¼å¼ï¼‰
            percentile = 2  # ä¸visualize_depthå‡½æ•°é»˜è®¤ç™¾åˆ†ä½ä¿æŒä¸€è‡´
            
            # ============== æ ¸å¿ƒä¿®å¤ï¼šå®Œå…¨å¯¹é½visualize_depthçš„å½’ä¸€åŒ–é€»è¾‘ ==============
            # æ­¥éª¤1ï¼šå¤åˆ¶æ•°æ®é¿å…ä¿®æ”¹åŸå§‹æ•°ç»„ï¼ˆä¸visualize_depthä¿æŒä¸€è‡´ï¼‰
            orb_depth_processed = valid_orb_slam_depth.copy()
            
            # æ­¥éª¤2ï¼šæœ‰æ•ˆæ·±åº¦å–å€’æ•°ï¼ˆvisualize_depthæ ¸å¿ƒé€»è¾‘ï¼šdepth[valid_mask] = 1 / depth[valid_mask]ï¼‰
            orb_valid_mask = orb_depth_processed > 0
            orb_depth_processed[orb_valid_mask] = 1 / orb_depth_processed[orb_valid_mask]
            
            # æ­¥éª¤3ï¼šè®¡ç®—ç™¾åˆ†ä½å¯¹åº”çš„min/maxï¼ˆä¸visualize_depthé€»è¾‘ä¸€è‡´ï¼‰
            if orb_valid_mask.sum() <= 10:
                orb_depth_min = 0
                orb_depth_max = 0
            else:
                orb_depth_min = np.percentile(orb_depth_processed[orb_valid_mask], percentile)
                orb_depth_max = np.percentile(orb_depth_processed[orb_valid_mask], 100 - percentile)
            
            # æ­¥éª¤4ï¼šé¿å…min/maxç›¸ç­‰ï¼ˆé˜²æ­¢é™¤é›¶é”™è¯¯ï¼Œä¸visualize_depthé€»è¾‘ä¸€è‡´ï¼‰
            if orb_depth_min == orb_depth_max:
                orb_depth_min = orb_depth_min - 1e-6
                orb_depth_max = orb_depth_max + 1e-6
            
            # æ­¥éª¤5ï¼šå½’ä¸€åŒ–åˆ°[0,1]èŒƒå›´ï¼ˆä¸visualize_depthé€»è¾‘ä¸€è‡´ï¼‰
            normalized_orb_depth = ((orb_depth_processed - orb_depth_min) / (orb_depth_max - orb_depth_min)).clip(0, 1)
            
            # æ­¥éª¤6ï¼šæ•°å€¼ç¿»è½¬ï¼ˆvisualize_depthæ ¸å¿ƒé€»è¾‘ï¼šdepth = 1 - depthï¼‰
            normalized_orb_depth = 1 - normalized_orb_depth
            # ============== å½’ä¸€åŒ–é€»è¾‘ä¿®å¤å®Œæˆ ==============
            
            # å…¼å®¹é«˜ç‰ˆæœ¬Matplotlibï¼Œè·å–plasmaè‰²å¡
            plasma_cmap = matplotlib.colormaps['plasma']  # ä¸DA3æ·±åº¦å›¾ç€è‰²è‰²å¡ä¿æŒä¸€è‡´
            
            for idx, (w, h) in enumerate(zip(valid_pixel_w, valid_pixel_h)):
                # 1. å…ˆç”»ç™½è‰²å®å¿ƒå¤–åœ†ï¼ˆä½œä¸ºè¾¹æ¡†ï¼Œé†’ç›®æ˜“è¯†åˆ«ï¼‰
                cv2.circle(
                    img=da3_depth_with_keypoints,
                    center=(w, h),
                    radius=outer_radius,
                    color=white_color,
                    thickness=-1  # å®å¿ƒå¡«å……
                )
                
                # 2. æå–å½“å‰ORB-SLAM3çœŸå®æ·±åº¦å¯¹åº”çš„RGBé¢œè‰²ï¼ˆæ ¸å¿ƒä¿®æ”¹ï¼šä»ORBæ•°æ®æå–ï¼‰
                norm_depth = normalized_orb_depth[idx]
                orb_rgb = plasma_cmap(norm_depth)[:3]  # è·å–plasmaè‰²å¡å¯¹åº”çš„RGBå€¼ï¼ˆ0~1èŒƒå›´ï¼‰
                orb_rgb_255 = (np.array(orb_rgb) * 255).astype(np.uint8)  # è½¬æ¢ä¸º0~255èŒƒå›´
                
                # ä¿®å¤coloréæ•°å€¼é”™è¯¯ï¼šè½¬æ¢ä¸ºPythonåŸç”Ÿæ•´æ•°å…ƒç»„
                orb_bgr = (
                    int(orb_rgb_255[2]),
                    int(orb_rgb_255[1]),
                    int(orb_rgb_255[0])
                )
                
                # 4. å†ç”»ORBçœŸå®æ·±åº¦è‰²å®å¿ƒå†…åœ†ï¼ˆä¸DA3æ·±åº¦å›¾è‰²å¡ä¸€è‡´ï¼Œä¾¿äºå¯¹æ¯”è¯¯å·®ï¼‰
                cv2.circle(
                    img=da3_depth_with_keypoints,
                    center=(w, h),
                    radius=inner_radius,
                    color=orb_bgr,
                    thickness=-1
                )
            
            # æ·»åŠ å…³é”®ç‚¹æ•°é‡æ ‡æ³¨
            cv2.putText(
                img=da3_depth_with_keypoints,
                text=f"Valid ROS Keypoints: {len(valid_pixel_w)}",
                org=(10, 30),
                fontFace=cv2.FONT_HERSHEY_SIMPLEX,
                fontScale=0.8,
                color=white_color,
                thickness=2
            )
    
    # æ­¥éª¤4ï¼šä¿å­˜å åŠ å…³é”®ç‚¹åçš„DA3æ·±åº¦å›¾
    da3_depth_keypoints_save_path = f"{DA3_DEPTH_WITH_KEYPOINTS_SAVE_PREFIX}{timestamp}.png"
    cv2.imwrite(da3_depth_keypoints_save_path, da3_depth_with_keypoints)
    print(f"[{timestamp}] ğŸ’¾ å åŠ ROSå…³é”®ç‚¹çš„DA3æ·±åº¦å›¾ä¿å­˜åˆ°: {da3_depth_keypoints_save_path}")

# ===================== æ–‡æœ¬ç”Ÿæˆæ¨¡å—ï¼ˆæ— ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰ =====================
def generate_text_stream(text_query: str, image_path: str, timestamp: str):
    """
    æµå¼ç”Ÿæˆæ–‡æœ¬å“åº”
    :param text_query: æ–‡æœ¬æŸ¥è¯¢æŒ‡ä»¤
    :param image_path: å›¾åƒä¿å­˜è·¯å¾„
    :param timestamp: æ—¶é—´æˆ³
    """
    global processor, model_vlm
    if not processor or not model_vlm:
        yield f"data: {json.dumps({'text': 'âŒ æ¨¡å‹æœªåŠ è½½å®Œæˆ'})}\n\n"
        return
    
    try:
        print(f"[{timestamp}] ğŸ¤– å¼€å§‹ç”Ÿæˆæ–‡æœ¬ï¼ŒæŸ¥è¯¢: '{text_query}'")
        
        # 1. æ„å»ºå¯¹è¯æ¶ˆæ¯
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "path": image_path},
                    {"type": "text", "text": text_query}
                ]
            },
        ]
        
        # 2. åº”ç”¨èŠå¤©æ¨¡æ¿å¹¶ç¼–ç 
        inputs = processor.apply_chat_template(
            messages,
            add_generation_prompt=True,
            tokenize=True,
            return_dict=True,
            return_tensors="pt",
        ).to(model_vlm.device, dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32)
        
        # 3. åˆå§‹åŒ–æµå¼ç”Ÿæˆå™¨
        streamer = TextIteratorStreamer(
            processor.tokenizer,
            skip_prompt=True,
            skip_special_tokens=True
        )
        
        # 4. æ„å»ºç”Ÿæˆå‚æ•°
        generation_kwargs = dict(
            inputs,
            streamer=streamer,
            max_new_tokens=MAX_NEW_TOKENS,
            do_sample=True,
            num_beams=1
        )
        
        # 5. å¯åŠ¨ç‹¬ç«‹çº¿ç¨‹æ‰§è¡Œç”Ÿæˆ
        thread = Thread(target=model_vlm.generate, kwargs=generation_kwargs)
        thread.start()
        
        # 6. æµå¼è¿”å›ç”Ÿæˆç»“æœ
        for new_text in streamer:
            if new_text:
                yield f"data: {json.dumps({'text': new_text})}\n\n"
        
        print(f"[{timestamp}] âœ… æ–‡æœ¬ç”Ÿæˆå®Œæˆ")
    except Exception as e:
        error_msg = f"æ–‡æœ¬ç”Ÿæˆå¤±è´¥: {str(e)}"
        yield f"data: {json.dumps({'text': f'âŒ {error_msg}'})}\n\n"

# ===================== Flaskæ¥å£æ¨¡å—ï¼ˆä»…é€‚é…å®¢æˆ·ç«¯è°ƒç”¨é‡æ„ï¼Œå…¶ä½™ä¸å˜ï¼‰ =====================
def init_flask_app():
    """
    åˆå§‹åŒ–Flaskåº”ç”¨
    """
    global app
    app = Flask(__name__)
    CORS(app)  # å…è®¸è·¨åŸŸè¯·æ±‚

    @app.route('/generate', methods=['POST'])
    def generate():
        # 1. è§£æè¯·æ±‚å‚æ•°
        data = request.json or {}
        text_query = data.get('text', 'Describe this image')
        timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
        
        try:
            # 2. æ¯æ¬¡è¯·æ±‚æ–°å»ºROSå®¢æˆ·ç«¯ï¼ˆé‡æ„ç‰ˆï¼Œæ— éœ€æå‰è¿æ¥ï¼‰
            print(f"[{timestamp}] ğŸ” æ–°å»ºROSå®¢æˆ·ç«¯å®ä¾‹...")
            ros_client = init_tracking_client(enable_log=False)
            if not ros_client:
                raise Exception("æ–°å»ºROSå®¢æˆ·ç«¯å®ä¾‹å¤±è´¥")
            
            # 3. ä¼ å…¥æ–°å»ºçš„ros_clientï¼Œä»ROSè·å–å›¾åƒå’Œç‚¹äº‘æ•°æ®ï¼ˆé€‚é…é—­ç¯æ–¹æ³•ï¼‰
            print(f"[{timestamp}] ğŸ” å¼€å§‹ä»ROSè·å–å›¾åƒå’Œç‚¹äº‘æ•°æ®...")
            pil_image, image_path, camera_point_cloud, world_point_cloud, cv_image = get_image_from_ros(ros_client, timestamp)
            if not pil_image:
                raise Exception(image_path)  # image_pathæ­¤æ—¶ä¸ºé”™è¯¯ä¿¡æ¯
            
            # åç»­é€»è¾‘ä¿æŒä¸å˜
            # 3. è·å–åŸå§‹å›¾åƒå°ºå¯¸ï¼ˆh, wï¼‰ï¼Œç”¨äºåŒ¹é…æ·±åº¦å›¾å°ºå¯¸
            image_shape = cv_image.shape[:2]  # (h, w)
            print(f"[{timestamp}] ğŸ“ åŸå§‹å›¾åƒå°ºå¯¸: {image_shape}ï¼Œå‡†å¤‡ç”Ÿæˆå¯¹åº”æ·±åº¦å›¾...")
            
            # 4. ç”ŸæˆDA3æ·±åº¦å›¾ï¼ˆåŒ¹é…åŸå§‹å›¾åƒå°ºå¯¸ï¼‰
            print(f"[{timestamp}] ğŸ“Š å¼€å§‹ç”ŸæˆDA3æ·±åº¦å›¾...")
            da3_depth_map = generate_depth_map(image_path, image_shape)
            
            # 5. ç»˜åˆ¶å¹¶ä¿å­˜æ·±åº¦å¯¹æ¯”å›¾ï¼ˆä¸¥æ ¼ä¸€ä¸€å¯¹åº”åƒç´ åæ ‡ï¼‰
            plot_depth_comparison(camera_point_cloud, da3_depth_map, timestamp, image_shape)
            save_da3_depth_with_ros_keypoints(da3_depth_map, camera_point_cloud, timestamp, image_shape)
            
            # 6. æµå¼è¿”å›ç”Ÿæˆç»“æœ
            return Response(generate_text_stream(text_query, image_path, timestamp), 
                            mimetype='text/event-stream')
        
        except Exception as e:
            error_msg = f"[{timestamp}] âŒ é”™è¯¯: {str(e)}"
            print(error_msg)
            return jsonify({'error': error_msg}), 500

# ===================== ä¸»ç¨‹åºå…¥å£ï¼ˆæ— ä¿®æ”¹ï¼Œä¿æŒåŸæœ‰é€»è¾‘ï¼‰ =====================
def main():
    """ä¸»ç¨‹åºï¼šåè°ƒå„æ¨¡å—åˆå§‹åŒ–ï¼Œå¯åŠ¨æœåŠ¡"""
    try:
        # 1. åˆå§‹åŒ–Flaskåº”ç”¨
        init_flask_app()
        
        # 2. åŠ è½½å„ç±»AIæ¨¡å‹
        load_model_vlm(MODEL_VLM_DEFAULT)
        load_model_da3(MODEL_DA3_DEFAULT)
        # load_model_sam3(MODEL_SAM3_PATH)
        
        # 3. å¯åŠ¨FlaskæœåŠ¡
        print(f"\n[{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}] ğŸš€ FlaskæœåŠ¡å¯åŠ¨ä¸­ï¼Œåœ°å€: http://{FLASK_HOST}:{FLASK_PORT}")
        app.run(
            host=FLASK_HOST,
            port=FLASK_PORT,
            threaded=True,
            debug=False  # ç”Ÿäº§ç¯å¢ƒå…³é—­debug
        )
    except Exception as e:
        print(f"[{datetime.datetime.now().strftime('%Y%m%d_%H%M%S')}] âŒ ç¨‹åºå¯åŠ¨å¤±è´¥: {str(e)}")
        os._exit(1)

if __name__ == '__main__':
    main()