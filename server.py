from flask import Flask, request, Response, jsonify
from flask_cors import CORS
from PIL import Image
from io import BytesIO
import requests
import torch
import json
import datetime
import os
import time
from threading import Thread

MODEL_QWEN_8B="Qwen/Qwen3-VL-8B-Instruct"
MODEL_QWEN_4B="Qwen/Qwen3-VL-4B-Instruct"
MODEL_QWEN_2B="Qwen/Qwen3-VL-2B-Instruct"
MODEL_SMOLVLM="HuggingFaceTB/SmolVLM2-2.2B-Instruct"

app = Flask(__name__)
CORS(app)

docker_ip = "localhost"
port = 51121

@app.route('/generate', methods=['POST'])
def generate():
    data = request.json
    text_query = data.get('text', 'Describe this image')
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    
    try:
        # èŽ·å–å›¾åƒ
        print(f"[{timestamp}] ðŸ” å¼€å§‹èŽ·å–å›¾åƒ...")
        image_response = requests.get(f"http://{docker_ip}:{port}/image")
        image_response.raise_for_status()
        print(f"[{timestamp}] âœ… æˆåŠŸèŽ·å–å›¾åƒï¼Œå¤§å°: {len(image_response.content)} bytes")
        
        # å¤„ç†å›¾åƒ
        pil_image = Image.open(BytesIO(image_response.content))
        print(f"[{timestamp}] ðŸ–¼ï¸ å›¾åƒå°ºå¯¸: {pil_image.size}, æ¨¡å¼: {pil_image.mode}")
        
        # ä¿å­˜å¤„ç†åŽçš„å›¾åƒ
        processed_path = f"image_{timestamp}.jpg"
        pil_image.save(processed_path)
        print(f"[{timestamp}] ðŸ’¾ ä¿å­˜å¤„ç†åŽå›¾åƒåˆ°: {processed_path}")
        
        def generate_stream():
            print(f"[{timestamp}] ðŸ¤– å¼€å§‹ç”Ÿæˆæ–‡æœ¬ï¼ŒæŸ¥è¯¢: '{text_query}'")
            
            # æž„å»ºæ¶ˆæ¯
            messages = [
                {
                    "role": "user",
                    "content": [
                        {"type": "image", "path": processed_path},
                        {"type": "text", "text": text_query}
                    ]
                },
            ]
            
            # åº”ç”¨æ¨¡æ¿
            inputs = processor.apply_chat_template(
                messages,
                add_generation_prompt=True,
                tokenize=True,
                return_dict=True,
                return_tensors="pt",
            ).to(model.device, dtype=torch.bfloat16)
            
            # åˆ›å»ºæµå¼ç”Ÿæˆå™¨
            streamer = TextIteratorStreamer(
                processor.tokenizer,
                skip_prompt=True,
                skip_special_tokens=True
            )
            
            # åœ¨å•ç‹¬çš„çº¿ç¨‹ä¸­è¿è¡Œç”Ÿæˆ
            generation_kwargs = dict(
                inputs,
                streamer=streamer,
                max_new_tokens=256,
                do_sample=True,
                num_beams=1
            )
            
            thread = Thread(target=model.generate, kwargs=generation_kwargs)
            thread.start()
            
            # ä»Žstreamerä¸­èŽ·å–ç”Ÿæˆçš„æ–‡æœ¬å¹¶å‘é€
            for new_text in streamer:
                if new_text:
                    # print(f"[{timestamp}] ðŸ“ æ–°æ–‡æœ¬: {new_text}")
                    yield f"data: {json.dumps({'text': new_text})}\n\n"
            
            print(f"[{timestamp}] âœ… ç”Ÿæˆå®Œæˆ")
            
        return Response(generate_stream(), mimetype='text/event-stream')
        
    except Exception as e:
        error_msg = f"[{timestamp}] âŒ é”™è¯¯: {str(e)}"
        print(error_msg)
        return jsonify({'error': error_msg}), 500


if __name__ == '__main__':
    from transformers import AutoProcessor, AutoModelForImageTextToText
    from transformers import TextIteratorStreamer
    
    model_path = MODEL_QWEN_4B
    processor = AutoProcessor.from_pretrained(model_path)

    print(f"{time.time()} > Loading model...", flush=True)
    model = AutoModelForImageTextToText.from_pretrained(
        model_path,
        # dtype=torch.bfloat16,
        load_in_8bit=True
    )

    print(f"{time.time()} > Compiling model...", flush=True)
    model = torch.compile(model)

    print(f"{time.time()} > Model is compiled!", flush=True)
    app.run(host='0.0.0.0', port=51122, threaded=True)