{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "efce563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e9b492422bc74a6089d4e4f27378de54",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoProcessor, AutoModelForImageTextToText\n",
    "import torch\n",
    "\n",
    "model_path = \"Qwen/Qwen3-VL-4B-Instruct\"\n",
    "processor = AutoProcessor.from_pretrained(model_path)\n",
    "model = AutoModelForImageTextToText.from_pretrained(\n",
    "    model_path,\n",
    "    # dtype=torch.bfloat16,\n",
    "    load_in_8bit=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd0d8472",
   "metadata": {},
   "outputs": [
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 34.79 GiB. GPU 0 has a total capacity of 10.57 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 8.58 GiB memory in use. Of the allocated memory 7.79 GiB is allocated by PyTorch, and 613.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOutOfMemoryError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 1493\u001b[39m\n\u001b[32m      1\u001b[39m messages = [\n\u001b[32m      2\u001b[39m     {\n\u001b[32m      3\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mrole\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33muser\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1482\u001b[39m     },\n\u001b[32m   1483\u001b[39m ]\n\u001b[32m   1485\u001b[39m inputs = processor.apply_chat_template(\n\u001b[32m   1486\u001b[39m     messages,\n\u001b[32m   1487\u001b[39m     add_generation_prompt=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1490\u001b[39m     return_tensors=\u001b[33m\"\u001b[39m\u001b[33mpt\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m   1491\u001b[39m ).to(model.device, dtype=torch.bfloat16)\n\u001b[32m-> \u001b[39m\u001b[32m1493\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1024\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1494\u001b[39m generated_texts = processor.batch_decode(\n\u001b[32m   1495\u001b[39m     generated_ids,\n\u001b[32m   1496\u001b[39m     skip_special_tokens=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m   1497\u001b[39m )\n\u001b[32m   1498\u001b[39m \u001b[38;5;28mprint\u001b[39m(generated_texts[\u001b[32m0\u001b[39m])\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/utils/_contextlib.py:120\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    118\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/generation/utils.py:2564\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[39m\n\u001b[32m   2561\u001b[39m model_kwargs[\u001b[33m\"\u001b[39m\u001b[33muse_cache\u001b[39m\u001b[33m\"\u001b[39m] = generation_config.use_cache\n\u001b[32m   2563\u001b[39m \u001b[38;5;66;03m# 9. Call generation mode\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2564\u001b[39m result = \u001b[43mdecoding_method\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   2565\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   2566\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2567\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlogits_processor\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_logits_processor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2568\u001b[39m \u001b[43m    \u001b[49m\u001b[43mstopping_criteria\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprepared_stopping_criteria\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2569\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m=\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2570\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mgeneration_mode_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2571\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   2572\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2574\u001b[39m \u001b[38;5;66;03m# Convert to legacy cache format if requested\u001b[39;00m\n\u001b[32m   2575\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   2576\u001b[39m     generation_config.return_legacy_cache \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m   2577\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(result, \u001b[33m\"\u001b[39m\u001b[33mpast_key_values\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   2578\u001b[39m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(result.past_key_values, \u001b[33m\"\u001b[39m\u001b[33mto_legacy_cache\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   2579\u001b[39m ):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/generation/utils.py:2784\u001b[39m, in \u001b[36mGenerationMixin._sample\u001b[39m\u001b[34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, streamer, **model_kwargs)\u001b[39m\n\u001b[32m   2781\u001b[39m model_inputs = \u001b[38;5;28mself\u001b[39m.prepare_inputs_for_generation(input_ids, **model_kwargs)\n\u001b[32m   2783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_prefill:\n\u001b[32m-> \u001b[39m\u001b[32m2784\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   2785\u001b[39m     is_prefill = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m   2786\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1344\u001b[39m, in \u001b[36mQwen3VLForConditionalGeneration.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, labels, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, logits_to_keep, **kwargs)\u001b[39m\n\u001b[32m   1314\u001b[39m \u001b[38;5;129m@check_model_inputs\u001b[39m()\n\u001b[32m   1315\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\n\u001b[32m   1316\u001b[39m     \u001b[38;5;28mself\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m   1329\u001b[39m     **kwargs: Unpack[TransformersKwargs],\n\u001b[32m   1330\u001b[39m ) -> Union[\u001b[38;5;28mtuple\u001b[39m, Qwen3VLCausalLMOutputWithPast]:\n\u001b[32m   1331\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1332\u001b[39m \u001b[33;03m    labels (`torch.LongTensor` of shape `(batch_size, sequence_length)`, *optional*):\u001b[39;00m\n\u001b[32m   1333\u001b[39m \u001b[33;03m        Labels for computing the masked language modeling loss. Indices should either be in `[0, ...,\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m   1342\u001b[39m \u001b[33;03m        TODO: Add example\u001b[39;00m\n\u001b[32m   1343\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1344\u001b[39m     outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1345\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1346\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1347\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpixel_values_videos\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1348\u001b[39m \u001b[43m        \u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mimage_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1349\u001b[39m \u001b[43m        \u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvideo_grid_thw\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1350\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1351\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1352\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1353\u001b[39m \u001b[43m        \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1354\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1355\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1356\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1358\u001b[39m     hidden_states = outputs[\u001b[32m0\u001b[39m]\n\u001b[32m   1360\u001b[39m     \u001b[38;5;66;03m# Only compute necessary logits, and do not upcast them to float if we are not computing the loss\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:1223\u001b[39m, in \u001b[36mQwen3VLModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, pixel_values, pixel_values_videos, image_grid_thw, video_grid_thw, cache_position, **kwargs)\u001b[39m\n\u001b[32m   1220\u001b[39m         position_ids = position_ids.add(delta)\n\u001b[32m   1221\u001b[39m         position_ids = position_ids.unsqueeze(\u001b[32m0\u001b[39m).expand(\u001b[32m3\u001b[39m, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1223\u001b[39m outputs = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlanguage_model\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1224\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1225\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1226\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1227\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1228\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1229\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1230\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvisual_pos_masks\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvisual_pos_masks\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1231\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdeepstack_visual_embeds\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdeepstack_visual_embeds\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1232\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1233\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1235\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Qwen3VLModelOutputWithPast(\n\u001b[32m   1236\u001b[39m     last_hidden_state=outputs.last_hidden_state,\n\u001b[32m   1237\u001b[39m     past_key_values=outputs.past_key_values,\n\u001b[32m   1238\u001b[39m     rope_deltas=\u001b[38;5;28mself\u001b[39m.rope_deltas,\n\u001b[32m   1239\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/generic.py:1072\u001b[39m, in \u001b[36mcheck_model_inputs.<locals>.wrapped_fn.<locals>.wrapper\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1069\u001b[39m                 monkey_patched_layers.append((module, original_forward))\n\u001b[32m   1071\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1072\u001b[39m     outputs = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1073\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m original_exception:\n\u001b[32m   1074\u001b[39m     \u001b[38;5;66;03m# If we get a TypeError, it's possible that the model is not receiving the recordable kwargs correctly.\u001b[39;00m\n\u001b[32m   1075\u001b[39m     \u001b[38;5;66;03m# Get a TypeError even after removing the recordable kwargs -> re-raise the original exception\u001b[39;00m\n\u001b[32m   1076\u001b[39m     \u001b[38;5;66;03m# Otherwise -> we're probably missing `**kwargs` in the decorated function\u001b[39;00m\n\u001b[32m   1077\u001b[39m     kwargs_without_recordable = {k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m kwargs.items() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m recordable_keys}\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:850\u001b[39m, in \u001b[36mQwen3VLTextModel.forward\u001b[39m\u001b[34m(self, input_ids, attention_mask, position_ids, past_key_values, inputs_embeds, use_cache, cache_position, visual_pos_masks, deepstack_visual_embeds, **kwargs)\u001b[39m\n\u001b[32m    848\u001b[39m \u001b[38;5;66;03m# decoder layers\u001b[39;00m\n\u001b[32m    849\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_idx, decoder_layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mself\u001b[39m.layers):\n\u001b[32m--> \u001b[39m\u001b[32m850\u001b[39m     layer_outputs = \u001b[43mdecoder_layer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    851\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    852\u001b[39m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    853\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtext_position_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    854\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    855\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    856\u001b[39m \u001b[43m        \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    857\u001b[39m \u001b[43m        \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    858\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    859\u001b[39m     hidden_states = layer_outputs\n\u001b[32m    861\u001b[39m     \u001b[38;5;66;03m# add visual features to the hidden states of first several layers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/modeling_layers.py:94\u001b[39m, in \u001b[36mGradientCheckpointingLayer.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     91\u001b[39m         logger.warning_once(message)\n\u001b[32m     93\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._gradient_checkpointing_func(partial(\u001b[38;5;28msuper\u001b[39m().\u001b[34m__call__\u001b[39m, **kwargs), *args)\n\u001b[32m---> \u001b[39m\u001b[32m94\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:502\u001b[39m, in \u001b[36mQwen3VLTextDecoderLayer.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, position_ids, past_key_values, use_cache, cache_position, **kwargs)\u001b[39m\n\u001b[32m    500\u001b[39m hidden_states = \u001b[38;5;28mself\u001b[39m.input_layernorm(hidden_states)\n\u001b[32m    501\u001b[39m \u001b[38;5;66;03m# Self Attention\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m502\u001b[39m hidden_states, _ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mself_attn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_ids\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    507\u001b[39m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m=\u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcache_position\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    509\u001b[39m \u001b[43m    \u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m=\u001b[49m\u001b[43mposition_embeddings\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    510\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    511\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    512\u001b[39m hidden_states = residual + hidden_states\n\u001b[32m    514\u001b[39m \u001b[38;5;66;03m# Fully Connected\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1773\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1771\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1772\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/torch/nn/modules/module.py:1784\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1779\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1780\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1781\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1782\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1783\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1784\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1786\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1787\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/utils/deprecation.py:172\u001b[39m, in \u001b[36mdeprecate_kwarg.<locals>.wrapper.<locals>.wrapped_func\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    168\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m minimum_action \u001b[38;5;129;01min\u001b[39;00m (Action.NOTIFY, Action.NOTIFY_ALWAYS) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m    169\u001b[39m     \u001b[38;5;66;03m# DeprecationWarning is ignored by default, so we use FutureWarning instead\u001b[39;00m\n\u001b[32m    170\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m172\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/models/qwen3_vl/modeling_qwen3_vl.py:444\u001b[39m, in \u001b[36mQwen3VLTextAttention.forward\u001b[39m\u001b[34m(self, hidden_states, position_embeddings, attention_mask, past_key_values, cache_position, **kwargs)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config._attn_implementation != \u001b[33m\"\u001b[39m\u001b[33meager\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    442\u001b[39m     attention_interface = ALL_ATTENTION_FUNCTIONS[\u001b[38;5;28mself\u001b[39m.config._attn_implementation]\n\u001b[32m--> \u001b[39m\u001b[32m444\u001b[39m attn_output, attn_weights = \u001b[43mattention_interface\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    445\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    446\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    447\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    448\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    449\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    450\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.0\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention_dropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    455\u001b[39m attn_output = attn_output.reshape(*input_shape, -\u001b[32m1\u001b[39m).contiguous()\n\u001b[32m    456\u001b[39m attn_output = \u001b[38;5;28mself\u001b[39m.o_proj(attn_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.conda/envs/torch/lib/python3.12/site-packages/transformers/integrations/sdpa_attention.py:96\u001b[39m, in \u001b[36msdpa_attention_forward\u001b[39m\u001b[34m(module, query, key, value, attention_mask, dropout, scaling, is_causal, **kwargs)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m attention_mask.dtype != torch.bool:\n\u001b[32m     93\u001b[39m         \u001b[38;5;66;03m# Convert to boolean type, making sdpa to force call FlashAttentionScore to improve performance.\u001b[39;00m\n\u001b[32m     94\u001b[39m         attention_mask = torch.logical_not(attention_mask.bool()).to(query.device)\n\u001b[32m---> \u001b[39m\u001b[32m96\u001b[39m attn_output = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mnn\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfunctional\u001b[49m\u001b[43m.\u001b[49m\u001b[43mscaled_dot_product_attention\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     97\u001b[39m \u001b[43m    \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     98\u001b[39m \u001b[43m    \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     99\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    100\u001b[39m \u001b[43m    \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    101\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdropout_p\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    102\u001b[39m \u001b[43m    \u001b[49m\u001b[43mscale\u001b[49m\u001b[43m=\u001b[49m\u001b[43mscaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    103\u001b[39m \u001b[43m    \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mis_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    104\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43msdpa_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    105\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    106\u001b[39m attn_output = attn_output.transpose(\u001b[32m1\u001b[39m, \u001b[32m2\u001b[39m).contiguous()\n\u001b[32m    108\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m attn_output, \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mOutOfMemoryError\u001b[39m: CUDA out of memory. Tried to allocate 34.79 GiB. GPU 0 has a total capacity of 10.57 GiB of which 1.46 GiB is free. Including non-PyTorch memory, this process has 8.58 GiB memory in use. Of the allocated memory 7.79 GiB is allocated by PyTorch, and 613.72 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "source": [
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"\"\"\n",
    "1.Nexitally旗下中转站\n",
    "Nexitally （中转机场）\n",
    "佩奇家主站，一家全线中转线路的高端机场，成立大概三年多了，机场主也比较佛系，机场是他们公司副业。全部线路已全部更换为 Anycast 接入，新增国内 Azure；新增 0.1 倍率节点 Hong Kong LB；新增华为云入口。\n",
    "\n",
    "稳定性和速度都不错，有自己的傻瓜客户端，可以登录后一键连接。价钱其实也不算太亲民，但是对于稳定追求性的用户也还好。\n",
    "\n",
    "稳定性不错，只要不是自己把自己折腾炸；多数线路改为 AIA 接入。新增游戏用加速器服务。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "科学上网服务：\n",
    "\n",
    "Smart Access：每月500G流量，112元/月，约577元/半年，约1049元/年；线路80+\n",
    "Smart Access Air：每月200G流量，55元/月，285元/半年，520元/年；限量提供；\n",
    "附加服务：Premium套餐有回国线路，需要单加钱，每月需要在基础套餐上+98元左右（看汇率）。比基础套餐多几条线路\n",
    "Group Access：允许至多5台设备（自家客户端）同时在线，每月+43元左右\n",
    "Business 服务（高优先级）：\n",
    "\n",
    "Lite：4999元/年，每月1.2T流量，15个在线设备支持；\n",
    "Pro：5999元/年，每月1.2T流量，30个在线设备支持；\n",
    "Unlimited：19999元/年，每月1.2T流量，无限设备支持。\n",
    "游戏加速器服务：\n",
    "\n",
    "Glowow加速器：月卡30元，双月卡52元，季卡69元，年卡229元。\n",
    "注意事项：\n",
    "\n",
    "软性限制ip（不过太多了后台也封）\n",
    "入口：广州移动、深圳BGP、苏州电信\n",
    "落地：Kirino、HKT、Eons、Jinx、Amazon、Kamatera、DigitalOcean等\n",
    "支持SS，SSR，支持Trojan，surge托管，支持Clash托管，支持SS-AEAD（单端口无obfs鉴权）\n",
    "购买Smart Access 31天赠3天NF会员；购买Smart Access 186天 赠7天NF会员；购买Smart Access 372天 赠30天NF会员；购买Premium Access 赠360天 NF、Disney+、Youtube Premium、Spotify会员任选\n",
    "支付方式：支付宝+微信\n",
    "网站（需挂代理访问）：https://bit.ly/479JoLM\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---Nexitally\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "AmyTelecom（中转机场）\n",
    "佩奇家分站，上海华为云 BGP 接入 + 阿里云深港专线，大概 50 个左右节点，速度一如既往的好；套餐相比奶昔流量更少但订阅价格也相对便宜一些，轻量版奶昔；已全部更换为 Anycast 接入，新增华为云入口。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Bronze：每月50G流量，273元/年；\n",
    "Sliver：每月150G，196元/半年，367元/年，690元/2年；\n",
    "Gold：每月300G流量，57.75元/月，294元/半年，551元/年，1030元/2年；\n",
    "Platinum：每月500G流量，435元/半年，810元/年；\n",
    "其他情况：\n",
    "\n",
    "不限制ip数和设备数\n",
    "入口：广东移动、广东联通、上海移动\n",
    "落地：香港 Jinx、Kirino、i-cable、Eons；台湾 Kirino、Prime Securities；新加坡 Prime Securities、Kirino；日本 Kirino；美国 Prime Securities、Kirino\n",
    "支持SS，支持surge，clash\n",
    "支付方式：支付宝+微信+QQ+Epay\n",
    "官网：https://bit.ly/477rsS1\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---AmyTelecom\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "2.ImmTel（中转机场）\n",
    "一家不错的大机场，大概 100 左右节点，线路已全部更换为 Anycast 接入，AIA，速度挺不错的，新增华为云入口。运营比较咸鱼。\n",
    "\n",
    "稳定性不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Basic (Personal)：208元/年，总300G流量；软性限制2ip。\n",
    "Basic Extended (Personal)：129元/季，每月200G流量；259元/半年；软性限制2ip。\n",
    "Standard (Personal)：72元/月，每月500G流量；206元/季，745元/年；软性限制2ip。\n",
    "Standard (Team Access)：103元/月，每月500G流量；296元/季，562元/半年；软性限制5ip。\n",
    "Standard Extended (Personal)：143元/月，每月1000G流量；409元/季，778元/半年；软性限制2ip。\n",
    "Standard Extended (Team Access)：177元/月，每月1000G流量；505元/季，960元/半年；软性限制10ip。\n",
    "其他情况：\n",
    "\n",
    "支持SS，支持surge，clash\n",
    "软性限制ip，短期偶尔可超，长期会断连接\n",
    "支付方式：支付宝+微信支付\n",
    "官网：https://bit.ly/479KxD4\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---ImmTel\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "3.FlowerCloud（中转机场）\n",
    "花云也是一家老牌大机场，在圈内有不错的用户口碑，深港 IEPL + 沪日 IEPL + 京德 IEPL，入口为国内多线 BGP 接入，落地资源丰富，另有 Star­link, Soft­Bank, UCOM, AT&T, Or­a­cle, Dig­i­talO­cean, Lin­ode, Zen­layer 等优质落地资源，流媒体解锁不错，香港 / 日本 / 新加坡 / 台湾 / 美国都可以稳定解锁 Dis­ney, HBO, Net­flix 和其他主流媒体资源。新增了很多原生冷门节点，实验性 0.2 倍率节点性价比很高。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Global Acceleration Air： 128元/一年，每月20G流量；仅限账户所有人本人。\n",
    "Global Acceleration Lite： 39/月，218元/半年，346/年，每月150G流量；仅限账户所有人本人。\n",
    "Global Acceleration Plus：58/月，318元/半年，546/年，每月400G流量；仅限账户所有人本人。\n",
    "Global Acceleration Max：113/月，618元/半年，946/年，每月1000G流量；仅限账户所有人本人。\n",
    "其他情况：\n",
    "\n",
    "支持SS/SSR/Trojan，支持surge，clash\n",
    "入口：上海电信、上海移动、广东联通入口\n",
    "落地：Starlink, So-Net, KDDI, AWS, Or­a­cle, Zenlayer, Dig­i­talO­cean, Lin­ode等\n",
    "元旦全场8折优惠码：QT960EVCGU ，2026年1月1日0:00开始到1月31日23:59，不包括Air/企业套餐\n",
    "支付方式：支付宝+微信支付+USDT\n",
    "官网：https://bit.ly/3GXSiVD\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---FlowerCloud\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "4.YToo（中转机场）\n",
    "开了快三年的一家机场，熟人推荐的，入口有广州，上海和北京，节点采用 IEPL 中转，有多国原生 IP 落地，提供 0.2 低倍率节点 ，流媒体稳定解锁 Net­flix，Dazn，Dis­ney 和其他主流媒体资源。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "AIR： 98元/年，每月15G流量；\n",
    "BASIC： 36元/月，219/半年，372元/年，每月200G流量；\n",
    "PLUS： 54元/月，328/半年，558元/年，每月400G流量；\n",
    "PRO： 108元/月，648元/半年，996元/年，每月1000G流量。\n",
    "其他情况：\n",
    "\n",
    "支持 SS/SSR/Trojan，支持 Surge，Clash等\n",
    "不限制ip或客户端\n",
    "元旦全场8.5折优惠码：R23ALY ，2026年1月1日00:00开始至2026年1月31日23:59结束，不包括Air/Team套餐\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3RMx8wl\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---YToo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "5.库洛米Kuromis（中转机场）\n",
    "开了一年多的站，定位高端；主打超大带宽低延迟与技术，全部节点支持 UDP；线路有深港专线，苏日专线，移动云等；所有技术自主研发，因为总会上新一些黑科技供用户使用，所以也会时不时爆炸，所以负责人又被群友们戏称为 Bug 昔\n",
    "\n",
    "稳定性不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "个人套餐：Base：40元/月，200G流量；Pro：66.9元/月，500G流量；Max：127.7元/月，1000G流量；限制2个IP\n",
    "家庭套餐：Base Family：48元/月，200G流量；Pro Family：80元/月，500G流量；Max Family：153元/月，1000G流量；限制5个IP\n",
    "企业套餐：Business Basis：361元/月，1200G流量，限制30个IP；Business Ultra：722元/月，2000G流量，限制999999个IP\n",
    "迷你套餐：Mini：32元/月，50G流量，限制2个IP；Mini Family：38元/月，50G流量，限制5个IP\n",
    "其他情况\n",
    "\n",
    "支持SS、Trojan、Https\n",
    "支付方式：目前支持支付宝，USDT即将开通\n",
    "官网：https://bit.ly/4aqnWVG\n",
    "\n",
    "\n",
    "（2026.01.02 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 库洛米 Kuromis\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "6.CreamData（中转机场）\n",
    "大佬介绍的一家性价比还不错的机场，100 + 线路，价钱还算比较亲民，移动 SD-WAN 线路。上海入口规划中，预计走同款 SD-WAN。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "青铜奶油加速：29元/月，每月200G流量；159元/半年，298元/年；允许2个同时在线数，限速200M\n",
    "黄金奶油加速：39元/月，每月300G流量；199元/半年，369元/年；允许2个同时在线数，限速500M\n",
    "铂金奶油加速：59元/月，每月600G流量；318元/半年，598元/年；允许2个同时在线数，限速1000M\n",
    "养老套餐(年付)：99元/年，总共220G流量，允许两个设备同时在线，限速200M，高速流量用尽后智能限速不断网\n",
    "其他情况：\n",
    "\n",
    "支持SS-AEAD/SSR/Trojan\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/47ZjWtC\n",
    "\n",
    "\n",
    "（2026.01.03 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---CreamData\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "7.LinkCube（中转机场）\n",
    "前身叫便利店，开了挺久的一家机场，节点广港专线 + 沪日专线 + 京德专线，入口为国内多线 BGP 接入。流媒体可稳定解锁 Dis­ney, HBO­MAX, Net­flix 等几乎所有主流流媒体资源。包含台湾、日本、美国等地优质家宽落地资源，海外带宽质量和速度也不错。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "轻量套餐：15元/月，每月100G流量；145元/年；\n",
    "标准套餐：30元/月，每月300G流量；160元/半年，280元/年；\n",
    "大流量套餐：90元/月，每月900G流量；460元/半年，860元/年；\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "支付方式：支付宝，微信支付，虚拟货币\n",
    "官网：https://bit.ly/3B5FN7e\n",
    "\n",
    "\n",
    "（2026.01.02 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---LinkCube\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "8.西部数据（中转机场）\n",
    "之前大佬推荐的一家机场，节点广港 IEPL / 沪日 IEPL / 京德 IEPL，流媒体稳定解锁 NF 和其他主流媒体资源，速度不错；华北和东北自动识别去京港京日，江浙沪自动识别去沪港，华南自动识别去广德。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "轻量套餐：20元/月，每月200G流量；仅限账户本人使用；\n",
    "标准套餐：40元/月，每月400G流量；仅限账户本人使用；\n",
    "专业套餐：60元/月，每月600G流量；仅限账户本人使用；\n",
    "极限套餐：80元/月，每月800G流量；仅限账户本人使用；\n",
    "其他情况：\n",
    "\n",
    "支持Trojan，支持Surge，Clash等\n",
    "支付方式：支付宝+USDT\n",
    "官网：https://bit.ly/475KsQI\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 西部数据\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "9.FlyingBird（中转机场）\n",
    "开了两三年多的一家不错的机场，看到其他频道的测速就拿来测了，还不错；对 Net­flix、Dis­ney + 等流媒体解锁支持度也很好；华为云广州 BGP 入口 + IEPL 广港 IEPL 专线 + 香港 BGP 出口。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "青铜套餐：15元/月，每月100G流量，41元/季，77元/半年，144年/月；不限速，不限制客户端数量\n",
    "白银套餐：30元/月，每月200G流量，81元/季，153元/半年，288年/月；不限速，不限制客户端数量\n",
    "黄金套餐：75元/月，每月500G流量，203元/季，383元/半年，720年/月；不限速，不限制客户端数量\n",
    "钻石套餐：150元/月，每月1000G流量，405元/季，765元/半年，1440元/年；不限速，不限制客户端数量\n",
    "其他套餐：\n",
    "\n",
    "支持SS\n",
    "入口：湛江移动、佛山电信、佛山移动、江门联通多组负载\n",
    "落地：Akari、FDCSERVERS、Kamatera等\n",
    "所有套餐季付9折，半年85折，年付8折\n",
    "圣诞元旦节日优惠：1: 月/季/半年付 85折 , 优惠码：fb250085 可重复使用5次；2: 年付8折（站内折上折，高达64折 ）优惠码：fb250080 可重复使用5次；活动时间：即日至2026年1月20日23点59分\n",
    "支付方式：支付宝+UDST\n",
    "官网：https://bit.ly/4oJDydt\n",
    "\n",
    "\n",
    "（2026.01.01 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---FlyingBird\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "10.ssrcloud （CNIX 中转机场）\n",
    "这家运营了大概四五年多了，也是合租香港家宽慢慢起步的，测了测速度相当不错，性价比不错，价钱还算实惠，有低价的轻量套餐。 200 + 节点，新增多条 BGP 线路，以及专线节点。\n",
    "\n",
    "稳定性还好，而且毕竟线路多；人数不少，不过速度还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "个人小流量标准版：14.99元.月，16G流量；180元/年，200G流量；350元/2年，400G；不限制客户端\n",
    "个人标准版：24.99元/月，204G流量；69.99元/季，768G流量；239.99元/年，2500G流量；不限制客户端\n",
    "个人增强版：34.99元/月，307G流量；349.99元/年，3584G流量；不限制客户端\n",
    "个人行业版：49.99元/月，512G流量；449.99元/年，4500G流量；不限制客户端\n",
    "家庭标准版：149.99元/月，1500G流量；599.99元/年，9000G流量；限制客户端4个\n",
    "小微企业标准版：3499元/年，35000G流量，限制10个客户端。\n",
    "需要注意：\n",
    "\n",
    "流量套餐不限制ip和设备数\n",
    "支持SS，SSR，V2ray，支持surge\n",
    "有用户分组，新用户无法获得全部的中转节点，需要用一段时间才可以\n",
    "支付方式：支付宝+微信+比特币\n",
    "官网：https://bit.ly/480VOqI\n",
    "\n",
    "\n",
    "（2026.01.02 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---ssrcloud\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "11.FastLink（中转机场）\n",
    "开了四年的老机场了，总体表现还不错，速度还可以，Any­Cast 加速技术，对于 Net­flix 、Dis­ney+ 流媒体及 Chat­GPT 的解锁表现比较不错；大概 100 左右线路。新增 AIA 和 IPLC 专线线路。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "基础版：20元/月，每月100G流量；57元/季，190元/年；\n",
    "加强版：40元/月，每月200G流量；114元/季，380元/年；\n",
    "专业版：80元/月，每月500G流量；228元/季，767元/年；\n",
    "至尊版：150元/月，每月1000G流量；428元/季，1440元/年。\n",
    "其他情况：\n",
    "\n",
    "支持SS，Vmess\n",
    "不限制连接数\n",
    "圣诞 & 元旦限时活动，全场8折专属优惠码：fastlink2026；活动时间：即日起 — 2026年1月20日 23:59，(配合站内年付自带8折，享受折上折，底至6.4折)\n",
    "支付方式：支付宝+微信支付\n",
    "官网：https://bit.ly/3Ju3ie9\n",
    "\n",
    "\n",
    "（2026.01.02 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---FastLink\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "12.一云梯（中转机场）\n",
    "朋友推荐的不错的新机场，刚开业没多久，暂时还有开业优惠比较划算，专线线路，有一些小众节点。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "VIP1套餐：15元/月，每月100G流量；41元/季，144元/年，324元/3年；\n",
    "VIP2套餐：30元/月，每月200G流量；81元/季，288元/年，648元/3年；\n",
    "VIP3套餐：60元/月，每月400G流量；171元/季，576元/年，1296元/3年；\n",
    "VIP3套餐：120元/月，每月1000G流量；324元/季，1152元/年，2592元/3年；\n",
    "DIY定制套餐：400元/月，专属地区独享IP独享带宽定制，适用于Tiktok直播，跨境电商平台卖家\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "不限制客户端数量，不限速\n",
    "季付9折，半年付85折，年付8折，2年付7折，3年付6折\n",
    "圣诞元旦节日优惠：1: 月/季/半年/年付 8折（配合站内折上折，高达6.4折）优惠码：25YD20OFF 可重复使用5次；2: 两年/三年付 75折（配合站内折上折，高达4.5折）优惠码：25YD25OFF 可重复使用5次。活动时间：即日起至2026年1月25日23点59分\n",
    "支付方式：支付宝，微信支付\n",
    "官网：https://bit.ly/4n3YqdU\n",
    "\n",
    "\n",
    "（2026.01.03 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 一云梯\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "13.Luckin（中转机场）\n",
    "熟人推荐的一家机场，入口有广州，上海，节点采用 IEPL 中转，有部分冷门地区 (待上线) 原生 IP 落地，流媒体稳定解锁 Net­flix，Dazn，Dis­ney 和其他主流媒体资源。\n",
    "\n",
    "稳定性不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Bronze计划：20元/月，每月200G流量，110元/半年，200元/年；最多10个IP同时连接使用\n",
    "Silver计划：50元/月，每月500G流量，268元/半年，498元/年；最多10个IP同时连接使用\n",
    "Gold计划：90元/月，每月950G流量，480元/半年，898元/年；最多10个IP同时连接使用\n",
    "Special计划：128元/年，总共200G流量；最多10个IP同时连接使用\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "支付方式：支付宝，微信支付，云闪付\n",
    "官网：https://bit.ly/49HL2Fz\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---Luckin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "14.泡芙云（中转机场）\n",
    "这家入手已经差不多两年多了，已经运行三四年多了，机场太多了，有时候都不知道怎么介绍了，自己看图吧，速度和性价比都还不错。最近新增一些内网线路，香港线路全线升级为 IEPL 专线接入。\n",
    "\n",
    "稳定性还可以，多数时候速度也不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "旗舰泡芙：35元/月，350元/年；每月500G流量，在线设备5台；增加内网专线\n",
    "旗舰泡芙Plus：60元/月，600元/年；每月1000G流量，在线设备5台；增加内网专线\n",
    "大泡芙：25元/月，249元/年，每月300G流量，在线设备5台。\n",
    "大泡芙Plus：35元/月，350元/年，每月700G流量，在线设备5台。\n",
    "小泡芙：15元/月，150元/年，每月150G流量；在线设备3台。\n",
    "轻量泡芙：8元/月，80元/年，每月50G流量；在线设备2台。\n",
    "旗舰团队：90元/月，900元/年；每月1.7T流量，不限制在线设备；增加内网专线\n",
    "团队泡芙：60元/月，600元/年；每月1.5T流量，不限制在线设备。\n",
    "其他情况：\n",
    "\n",
    "支持SSR，SS，Clash，Surge，Surfboard\n",
    "新年优惠：所有套餐 8 折优惠，优惠码：2026\n",
    "\n",
    "支付方式：支付宝+银联\n",
    "官网：https://bit.ly/480Yp3W\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 泡芙云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "15.青云梯（中转机场）\n",
    "运营第四年的一家机场了，专线内网传输线路，最高 4Gbps 速率，团队为新加坡海外团队，线路解锁也还不错，速度也比较稳定，其他的后续再观察。联通增加了 10G 带宽。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "青云梯：96/年付，每月60G流量；8元可重置；\n",
    "青云VIP1：25/月付，每月150G流量；67/季付，240/年付；\n",
    "青云VIP2：45/月付，每月300G流量；121/季付，432/年付；\n",
    "青云VIP3：85/月付，每月600G流量；229/季付，916/年付；\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "入口：广东移动多组负载\n",
    "落地：Akari、Nearoute、CHOOPA、Hytron、Cogent、Fiber等\n",
    "不限制设备数在线，同时支持多个IP在线\n",
    "全场八折优惠码：0080 ，持续到1月30\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/4cPVMDE\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 青云梯\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "16.KyCloud（老中转机场）\n",
    "也是一家老机场，六七年了，还一直在，线路比较多，种类比较全，总体表现还不错，多条线路支持 NF，广州电信、江门联通、广州移动动态解析。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Mini：每月10G流量，150元/年。\n",
    "Basic：每月50G流量，75元/季，248元/年。\n",
    "Sliver：每月100G流量，90元/季，298元/年。\n",
    "Platinum：每月200G流量，40元/月，398元/年。\n",
    "Ultimate1：每月300G流量，150元/季，498元/年。\n",
    "Ultimate2：每月500G流量，210元/季，698元/年。\n",
    "Team1：每月1T流量，369元/季，1230元/年。\n",
    "Team2：每月2T流量，609元/季，1998元/年。\n",
    "需要注意：\n",
    "\n",
    "支持SS，V2Ray\n",
    "入口：厦门联通、深圳移动、广州电信、深圳联通\n",
    "落地：香港 Akari、LSHIY、Polo；台湾 Hinet、Akari；日本 Akari、Vultr；新加坡 Speedy、Akari；美国 WebNX；越南 VPNT；德国 Hetzner等\n",
    "不限制设备连接数\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/41xd5Fl\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---KyCloud\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "17.咸鱼加速器nexuscloud（中转机场）\n",
    "近期发现的一家中转机场，移动 IEPL 线路，流媒体稳定解锁 NF 和其他主流媒体资源，速度不错。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Standard：20元/月，每月100G流量；108元/半年，198元/年；限账户所有人本人使用\n",
    "Standard Extended：40元/月，每月300G流量；218元/半年，388元/年；限账户所有人本人使用\n",
    "其他情况：\n",
    "\n",
    "支持SS/SSR，支持Surge，Clash等\n",
    "入口：广东联通\n",
    "落地：香港、日本、新加坡、美国 Kirino；台湾Akari\n",
    "支付方式：支付宝+USDT\n",
    "官网：https://bit.ly/3TyUS8r\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 咸鱼加速器\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "18.龙猫云（中转机场）\n",
    "已经运营半年多的一家机场，速度还算不错，流媒体解锁比较全面，其他再慢慢观察。可支持企业和个人独享 IP 和家宽定制，站长早已移民海外。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "VIP1套餐：15元/月，每月100G流量，41元/季，144元/年；\n",
    "VIP2套餐：30元/月，每月200G流量，81元/季，288元/年；\n",
    "VIP3套餐：60元/月，每月400G流量，171元/季，576元/年；\n",
    "VIP4套餐：120元/月，每月1000G流量，324元/季，1152元/年；\n",
    "DIY定制套餐：400元/月，1140元/季，3840元/年；专属地区独享IP独享带宽定制\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "不限制客户端\n",
    "支付方式：支付宝，微信支付\n",
    "官网：https://bit.ly/491wbct\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 龙猫云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "19.魅影旗下中转站\n",
    "魅影极速（中转机场）\n",
    "也是一家老牌机场了，开了多年了，服务器是内网专线中转节点，广州移动入口，总体还不错，站长人在新加坡。但是阿里云入口这个特色没了，大厂落地也偏多。\n",
    "\n",
    "稳定性不错，只要不自己把自己折腾炸了。前段时间被攻击，还没恢复到最佳状态。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "VIP3：季付180元，每月400G流量；年付600元；每月500G流量；峰值带宽100M，节点40+左右；\n",
    "VIP4：半年付480元，每月800G流量；年付800元；每月900G流量；峰值带宽200M，节点100+。\n",
    "需要注意的：\n",
    "\n",
    "限制6个连接数，目前仅支持SSR，支持新版Clash；不支持Surge；\n",
    "支付方式：卡密充值（支付宝+微信+网银）\n",
    "官网：https://bit.ly/498x2EY\n",
    "\n",
    "\n",
    "（2024.09.22 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 魅影极速\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ARK（SS中转机场）\n",
    "魅影家新开的一个分站，主要是三网隧道，速度还不错。可能也是做主站的试验田吧，大概 50 个节点。\n",
    "\n",
    "稳定性还可以\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "A1季度订阅：150元，90天，每月200G流量；客户端数量4个；\n",
    "A1半年订阅：275元，180天，每月200G流量；客户端数量4个；\n",
    "A1年度订阅：500元，360天，每月200G流量；客户端数量4个。\n",
    "其他情况：\n",
    "\n",
    "支持SS，支持Surge，支持clash\n",
    "卡密充值（支付宝+微信+网银）\n",
    "官网： https://bit.ly/3GMbGRO\n",
    "\n",
    "\n",
    "（2024.04.27 晚 9 点测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---ARK\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "20.V2Tun（中转小机场）\n",
    "YToo 分站，入口有广州，上海，节点采用 IEPL 中转，有部分冷门地区原生 IP 落地，流媒体稳定解锁 Net­flix，Dazn，Dis­ney 和其他主流媒体资源。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Mini套餐：36元/季，每月100G流量；\n",
    "Bronze套餐：54元/季，每月200G流量；\n",
    "Silver套餐：84元/季，每月500G流量；\n",
    "Gold套餐：144元/季，每月1000G流量。\n",
    "其他情况：\n",
    "\n",
    "支持Trojan，支持Surge，Clash\n",
    "元旦全场8.5折优惠：NAEG6S ，至2026年1月31日23:59结束\n",
    "支付方式：聚合支付（微信支付）\n",
    "官网：https://bit.ly/48epVuq\n",
    "\n",
    "\n",
    "（2026.01.04 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---V2Tun\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "21.酷酷云KuKuYun（中转机场）\n",
    "2025 年成立的一家机场，Tro­jan 协议，IEPL 专线节点，BGP 入口，三网体验都不差，节点负载均衡，解锁 Net­flix、Dis­ney+ 流媒体和 AI 工具 Chat­GPT 等。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "VIP1套餐：20元/月，每月100G流量；54元/季，192元/年；不限速\n",
    "VIP2套餐：40元/月，每月200G流量；108元/季，384元/年；不限速\n",
    "VIP3套餐：90元/月，每月500G流量；243元/季，864元/年；不限速\n",
    "VIP4套餐：1600元/月，每月1000G流量；432元/季，1536元/年；不限速\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/456LyNY\n",
    "\n",
    "\n",
    "（2026.01.05 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 酷酷云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "22.Nerwo奶瓶（中转机场）\n",
    "19 年开的一家机场，通常也称呼为奶瓶，可能是老板也喜欢收集冷门节点，这方面的节点不少。缺点就是 Lin­ode、Or­a­cle 等大厂落地占比过高，移动入口。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Light轻量：37元/月，90元/季，每月90G流量，限制3台设备；300元/年，双倍流量+设备；可使用少量节点，限速90M；\n",
    "Premium高级：42元/月，110元/季，每月300G流量，限制4台设备；360元/年，双倍流量+设备；可使用部分节点，限速300M；\n",
    "Exclusive专业：55元/月，135元/季，每月600G流量，限制5台设备；500元/年，1.5倍流量+设备；可使用全部节点，不限速；\n",
    "Special特别：240元/年，每年300G流量；310元/2年，双倍流量；550元/3年，5倍流量；限制3台设备，限速200M；\n",
    "Company企业：1500元/月，3000元/季，每月10240G流量，限制10台设备；8000元/年，双倍流量+设备；可使用全部节点，不限速；\n",
    "其他情况：\n",
    "\n",
    "支持SSR，SS，支持Surge，clash\n",
    "支付方式：支付宝+加密货币\n",
    "官网：https://bit.ly/41uTQfI\n",
    "\n",
    "\n",
    "（2026.01.05 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---Nerwo\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "23.次元链接（中转机场）\n",
    "也是一家老牌大站了，几年前在 youtube 上见过，后来试了试还可以就开始测了直到现在。新增多条内网线路和中转线路。有部分低倍率和家宽 IP 节点提供，并提供了小白友好的定制三端支持。\n",
    "\n",
    "稳定性还好。被 N3RO 收购，独立运营。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "狂热套餐：48.8元/月，每月428G流量；138元/季，498元/年；限速500M，非异地5人使用；\n",
    "大众套餐：28.8元/月，每月228G流量；82元/季，292元/年；限速300M，仅限购买者本人使用；\n",
    "入门套餐：18.8元/月，每月128G流量；52元/季，192元/年；限速200M，仅限购买者本人使用；\n",
    "注意事项：\n",
    "\n",
    "支持surge托管，clash托管，支持SS/SSR\n",
    "入口：四川移动、湖北联通、安徽移动多组负载\n",
    "落地：Akari、Vultr、Chunghwa Telecom 等\n",
    "支付方式：支付宝+微信\n",
    "官网：https://bit.ly/49QIAhE\n",
    "\n",
    "\n",
    "（2026.01.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 次元链接\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "24.小旋风（中转机场）\n",
    "新开业没多久的一家站，线路解锁还不错，速度也可以，流媒体解锁比较全面；支持企业级定制线路。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "普通套餐：20元/月，每月110G流量；54元/季，102元/半年，192元/年；最大速率500M\n",
    "高级套餐：40元/月，每月220G流量；108元/季，204元/半年，384元/年；最大速率500M\n",
    "专业套餐：80元/月，每月440G流量；216元/季，408元/半年，768元/年；最大速率1000M\n",
    "企业套餐：200元/月，每月1200G流量；540元/季，1020元/半年，1920元/年；最大速率1000M\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "不限制设备数量\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/4fThXLO\n",
    "\n",
    "\n",
    "（2026.01.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 小旋风\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "25.闪狐云（中转机场）\n",
    "朋友介绍的一家站，采用 TLS 协议加密技术，通过五大运营商 BGP+IPLC 动态优化和国际多个出口节点，不限制 IP 和设备数量，速度还不错。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Basic Plan：20元/月，每月120G流量；54元/季，102元/半年，192元/年；\n",
    "Standard Plan：40元/月，每月240G流量；108元/季，204元/半年，384元/年；\n",
    "Advanced Plan：72元/月，每月500G流量；194元/季，367元/半年，691元/年；\n",
    "Premium Plan：125元/月，每月1000G流量；337元/季，637元/半年，1200元/年。\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "不限制IP和设备数量\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/4167Pdu\n",
    "\n",
    "\n",
    "（2026.01.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 闪狐云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "26.银河云GalaxyCloud（中转机场）\n",
    "银河云 tro­jan 专线机场，成立与 2024 年，新加坡海外团队，性价比不错；对 Net­flix，Chat­GPT，Dis­ney+，Tik­Tok 等解锁支持还不错。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "轻量用户年付礼包套餐：98元/年，每月50G流量；\n",
    "星尘套餐：18元/月，每月100G流量；\n",
    "行星套餐：35元/月，每月200G流量；\n",
    "恒星套餐：70元/月，每月400G流量；\n",
    "星系套餐：140元/月，每月800G流量；\n",
    "1T流量不限时套餐：680元，总1T流量，用完截止\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "季付9折，季付85折，半年付8折，俩年付7折，三年付6折\n",
    "不限制客户端\n",
    "支付方式：支付宝，微信，USDT\n",
    "官网：https://bit.ly/45aXEqp\n",
    "\n",
    "\n",
    "（2026.01.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 银河云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "27.飞天猪（中转机场）\n",
    "去年初成立的一家机场，东南亚的团队，比较低调，入口有深圳，上海，安徽等，内网专线，速度和流媒体解锁都还不错。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Fliggy-轻量年包：108元/年，每月60G流量；\n",
    "Fliggy-VIP1：20元/月，每月120G流量；54元/季，192元/年；\n",
    "Fliggy-VIP2：40元/月，每月240G流量；108元/季，384元/年；\n",
    "Fliggy-VIP3：70元/月，每月500G流量；189元/季，672元/年；\n",
    "Fliggy-VIP4：120元/月，每月1000G流量；324元/季，1152元/年；\n",
    "其他情况：\n",
    "\n",
    "支持Trojan，V2Ray\n",
    "无设备限制\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3ZdKPbO\n",
    "\n",
    "\n",
    "（2025.11.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 飞天猪\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "28.速云梯（中转机场）\n",
    "速鹰家分站，主要是 BGP 节点和专线，以及其他一些国内中转线路，套餐性价比还可以。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "入门版：15.9元/月，每月100G流量；3个设备同时使用，限速60M；国内中转节点；\n",
    "基础版：25.9元/月，每月200G流量；5个设备同时使用，限速150M；国内中转节点+基础版节点；\n",
    "标准版：35.9元/月，每月350G流量；8个设备同时使用，限速300M；国内中转节点+基础版节点+标准版节点；\n",
    "旗舰版：55.9元/月，每月600G流量；16个设备同时使用，不限速；国内中转节点+基础版节点+标准版节点+专线节点；\n",
    "其他情况：\n",
    "\n",
    "支持SSR，Vmess\n",
    "支付方式：支付宝+微信支付\n",
    "官网：https://bit.ly/474fzMv\n",
    "\n",
    "\n",
    "（2025.10.09 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 速云梯\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "29.飞机云（中转机场）\n",
    "一家新开没多久的机场，测了几个月感觉还不错就放上来了，主要是普通中转隧道和 IPLC 专线，节点数量不少，解锁比较全。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "入门版：9.99元/月，每月50G流量；1个设备同时使用，限速60M；入门版节点+中转\n",
    "基础版：15.99元/月，每月100G流量；2个设备同时使用，限速60M；基础版节点+中转\n",
    "标准版：25.99元/月，每月200G流量；3个设备同时使用，限速150M；标准版节点+中转\n",
    "高级版：35.99元/月，每月350G流量；5个设备同时使用，限速300M；高级版节点+中转\n",
    "旗舰版：49.99元/月，每月600G流量；8个设备同时使用，不限速；旗舰版节点+专线\n",
    "其他情况\n",
    "\n",
    "支持SSR，V2ray\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3GOWIuo\n",
    "\n",
    "\n",
    "（2025.11.05 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 飞机云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "30.少数派（中转机场）\n",
    "入手三年的老机场了，此少数派非彼少数派；多数线路是阿里和移动电信中转，总共 40 + 线路，速度还不错。\n",
    "\n",
    "稳定性还算不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "个人版200G：29元/月，86元/季，329元/年，每月200G流量；限制3个客户端在线；\n",
    "个人版300G：45元/月，129元/季，519元/年，每月300G流量；限制3个客户端在线；\n",
    "团队版600G：89元/月，259元/季，938元/年，每月600G流量；限制10个客户端在线；\n",
    "个人版全年1200G：258元/年，总1200G流量；限制3个客户端在线；\n",
    "个人版全年2000G：388元/年，总2000G流量；限制3个客户端在线；\n",
    "团队版全年4000G：698元/年，总4000G流量；限制10个客户端在线；\n",
    "其他情况：\n",
    "\n",
    "支持SS，支持Vmess\n",
    "支付方式：支付宝+微信\n",
    "官网：https://bit.ly/3TuRTOv\n",
    "\n",
    "\n",
    "（2025.11.05 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 少数派\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "31.白月光（中转机场）\n",
    "拿在手里也有段时间的一家机场了，2021 年开业到现在，测了很多次，IEPL 专线节点，负载均衡，有一次性流量包套餐。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "小包：66元/季，每月140G流量；132元/半年，264元/年；限制5个公网IP\n",
    "中包：84元/季，每月230G流量；168元/半年，336元/年；限制5个公网IP\n",
    "大包：120元/季，每月400G流量；240元/半年，480元/年；限制5个公网IP\n",
    "超大包：180元/季，每月750G流量；360元/半年，720元/年；限制5个公网IP\n",
    "不差钱：210元/季，每月900G流量；420元/半年，840元/年；限制5个公网IP\n",
    "旗舰包：2500元/年，每月2500G流量；5000元/2年；限制5个公网IP\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "支付方式：支付宝+微信\n",
    "官网：https://bit.ly/3TsS1Ok\n",
    "\n",
    "\n",
    "（2025.11.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 白月光\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "32.TKV Network（SS中转机场）\n",
    "一家全中转 SS 协议机场，，因为名字比较像 KTV，所以又被称为 KTV。线路主要是广港 IEPL、沪日 IEP、京德 IEPL，节点前置江苏电信高防入口、广东联通高防入口。节点配置流媒体解锁，并附带 Telegram DC5 的专属 SGGS 线路优化。落地节点有香港 Mis­akaF、Stacks，澳门 CTM，台湾 Akari，日本 Akari、IIJ，新加坡 Singtel、Akari、Speedy­Page，韩国 KT，美国 Mis­aka、Stacks，德国 Het­zner、Mis­aka，荷兰 Mis­aka。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Lite套餐：月付20，每月流量100G；54元/季，102元/半年，192元/年；\n",
    "Medium套餐：月付50，每月流量250G；135元/季，255元/半年，480元/年；\n",
    "Large套餐：月付80，每月流量500G；216元/季，408元/半年，768元/年；\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "入口：深圳联通、厦门快快网络\n",
    "落地：香港 MisakaF、Stacks；日本 Akari、IIJ；新加坡 Sintel、SpeedyPage、Akari、Eons；澳门 CTM；台湾 Akari；德国 Hetzner、Misaka；美国 Stacks、Misaka等\n",
    "动态倍率，0:00—8:00： 0.2倍率；8:00—18:00： 0.5倍率；18:00—24:00：1.0倍率\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3LVtgFJ\n",
    "\n",
    "\n",
    "（2026.01.02 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---TKV Network\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "33.云翼网络（中转机场）\n",
    "原辉耀网络，之前测过几次速，还不错，主要是移动中转和 AZ 中转，三个等级线路一样，流量不一样；已更换 IEPL 线路。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "标准会员：22.8元/月，66.8元/季，248元/年，每月88G流量，4个在线客户端；\n",
    "高级会员：32.8元/月，96.8元/季，368元/年，每月168G流量，4个在线客户端；\n",
    "终极会员：89.8元/月，268.8元/季，528元/半年，每月488G流量，6个在线客户端。\n",
    "全年648G会员，168-188元，3个客户端；全年1398G会员，328元，4个客户端；全年3288G会员，688元，6个客户端。\n",
    "其他情况：\n",
    "\n",
    "支持SS，Vmess\n",
    "支付方式：支付宝+微信\n",
    "官网：https://bit.ly/4aozrgl\n",
    "\n",
    "\n",
    "（2025.11.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 云翼网络\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "34.FacMata（中转机场）\n",
    "前段时间发现的一家站，已经开业一年多了，试了试感觉还可以就放上来了，有一些冷门线路，速度还可以，再继续观察下。\n",
    "\n",
    "稳定性还不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Standard Lite：9.9元/月，120G流量\n",
    "Standard：19.9元/月，每月300G流量；59.9元/季，239元/年；\n",
    "Standard Pro：39.9元/月，每月800G流量；119元/季，479元/年；\n",
    "其他情况：\n",
    "\n",
    "支持Vmes\n",
    "支付方式：支付宝+虚拟货币\n",
    "官网：https://bit.ly/49JxSIF\n",
    "\n",
    "\n",
    "（2025.11.07 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---FacMata\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "35.Aladdin（中转机场）\n",
    "Aladdin（中转机场）\n",
    "是一家 2024 年开业的性价比中转机场，公网隧道中转节点，三网专线，节点包含了五大常用地区及超多冷门地区节点提供。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Promotional 100G：15元/年，总100G流量；限制2个IP；节点分组 : Premium User\n",
    "Promotional 300G：48元/半年，总300G流量，90元/年；限制5个IP；节点分组 : VIP\n",
    "VIP 138G：10元/月，每月138G流量，30元/季，120元/年；限制15个IP；节点分组 : VIP\n",
    "VIP 288G：18元/月，每月288G流量，54元/季，216元/年；限制15个IP；节点分组 : VIP\n",
    "VIP 588G：30元/月，每月588G流量，180元/季，360元/年；限制15个IP；节点分组 : VIP\n",
    "VIP 1288G：50元/月，每月1288G流量，300元/季，600元/年；限制15个IP；节点分组 : VIP\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "半年付优惠码：SemiAnnually ；年付优惠码：Annually\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3TCLZsX\n",
    "\n",
    "\n",
    "（2025.11.06 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---Aladdin\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "KooDog（中转机场）\n",
    "Al­addin 旗下的一家机场，同是三网中转，自研客户端目前有 IOS，安卓和 Win­dows，Ma­cos，适合小白使用，速度还不错。目前只开放客户端。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Edge Lightsail：5元/月，每月30G流量；\n",
    "Edge：15元/月，每月100G流量；\n",
    "Lightsail：15元/月，每月40G流量；送Emby等账号福利；流量结转\n",
    "Basic：30元/月，每月140G流量；送Emby等账号福利；流量结转\n",
    "Premium：50元/月，每月280G流量；送Emby等账号福利；流量结转\n",
    "Pro：100元/月，每月580G流量；送Emby等账号福利；流量结转\n",
    "其他情况：\n",
    "\n",
    "支持SS，Trojan；暂时只开放客户端使用\n",
    "支付方式：支付宝+USDT\n",
    "官网：https://bit.ly/3J490Dw\n",
    "\n",
    "\n",
    "（2025.10.18 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "36.V2Paw（中转机场）\n",
    "2025 年新开业的小众机场，Vmess 协议，多运营商入口，公网隧道中转，有少量直连优化线路，三网用户都能有合适的线路节点可选，节点解锁 Net­flix、Dis­ney+ 流媒体和 AI 工具 Chat­GPT 等。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "个人套餐 - 基础款：15元/月，每月100G流量；45元/季，144元/年；\n",
    "个人套餐 - 标准款：25元/月，每月200G流量；71元/季，240元/年；\n",
    "个人套餐 - 高级款：50元/月，每月500G流量；142元/季，480元/年；\n",
    "个人套餐 - 豪华款：100元/月，每月125G流量；285元/季，960元/年；\n",
    "其他情况：\n",
    "\n",
    "支持Vmess\n",
    "支付方式：支付宝，微信\n",
    "官网：https://bit.ly/4qhiooc\n",
    "\n",
    "\n",
    "（2026.01.06 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "37.Xlinkworld（中转机场）\n",
    "开了差不多 3 年多的一家机场，总体来说还算可以。最近全部更换了 IEPL 线路。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "旗舰加速：20元/月100G，25元/月150G，40元/月250G，55元/月400G，80元/月600G，105元/月1000G；\n",
    "经济加速：120元/年，每月50G流量；\n",
    "其他情况：\n",
    "\n",
    "不限制IP和客户端\n",
    "支持SSR\n",
    "优惠码：xlinkworld ，全场套餐任意周期一次性8折优惠\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/477HFql\n",
    "\n",
    "\n",
    "（2025.11.07 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---XLinkWorld\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "38.ssLinks（中转机场）\n",
    "也是在手里一段时间的一家机场了，多次测速还可以就放上来；主要是移动中转。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Mini：78元/季，每月80G流量；允许三个设备同时在线；\n",
    "Basic：36元/月，每月260G流量；允许三个设备同时在线；\n",
    "Pro：56元/月，每月600G流量；允许三个设备同时在线；\n",
    "Premium：68元/月，每月1024G流量；允许三个设备同时在线；\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/479k9cq\n",
    "\n",
    "\n",
    "（2025.11.07 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 ---SSLinks\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "39.大哥云（中转机场）\n",
    "也是一家开业几年的机场，公网隧道和 IPLC 专线节点，有部分节点为原生家宽 IP 节点。解锁方面，大部分常用该地区节点支持 Net­flix、Dis­ney+ 流媒体解锁。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "小流量年付：88元/年，每月15G流量；限速500M\n",
    "年付套餐A：199元/年，每月300G流量；限速1000M\n",
    "年付套餐B：299元/年，每月500G流量；限速1000M\n",
    "季付套餐A：69元/季，每月200G流量；限速1000M\n",
    "单月套餐B：29元/月，每月150G流量；限速500M\n",
    "单月套餐A：19元/月，每月100G流量；限速500M\n",
    "其他情况：\n",
    "\n",
    "支持Trojan\n",
    "入口：广东移动\n",
    "落地：Akari、HGC、Polo Network、Oracle 等\n",
    "支付方式：微信，支付宝，USDT\n",
    "官网：https://bit.ly/3tkn1FJ\n",
    "\n",
    "\n",
    "（2025.11.13 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "40.猫熊网络（中转机场）\n",
    "也是在手里拿了两年多的一家机场，每月测一次，一直还算稳定就放上来了。\n",
    "\n",
    "稳定性还行。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "普通版VIP1套餐：6元/月，每月35G流量；18元/季，72元/年；2个在线客户端；基础节点\n",
    "豪华版VIP2套餐：12元/月，每月100G流量；30元/季，60元/半年；2个在线客户端；国际标准节点\n",
    "豪华版大流量VIP2套餐：23元/月，每月300G流量；69元/季，276元/年；2个在线客户端；国际标准节点\n",
    "专线50GVIP3套餐：13元/月，每月500G流量；39元/季，156元/年；3个在线客户端；专线+国际节点；限速50M\n",
    "专线200GVIP3套餐：25元/月，每月200G流量；70元/季，263元/年；4个在线客户端；专线+国际节点；不限速\n",
    "专线300GVIP3套餐：35元/月，每月300G流量；100元/季，362元/年；4个在线客户端；专线+国际节点；不限速\n",
    "专线500GVIP3套餐：48元/月，每月500G流量；135元/季，504元/年；5个在线客户端；专线+国际节点；不限速\n",
    "其他情况：\n",
    "\n",
    "支持V2Ray\n",
    "支付方式：支付宝+微信+加密货币\n",
    "官网：https://bit.ly/3RO6cfB\n",
    "\n",
    "\n",
    "（2025.11.13 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "41.蓝帆云（中转机场）\n",
    "朋友推荐的一家机场，速度还可以，还有些冷门线路，主要是移动线路，还有些专线。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "入门版：9.99元/月，每月50G流量，限制1个设备，限速60M；可用入门版节点\n",
    "基础版：15.9元/月，每月100G流量，限制2个设备，限速60M；可用基础版节点\n",
    "标准版：25.9元/月，每月200G流量，限制3个设备，限速150M；可用标准版节点\n",
    "高级版：35.9元/月，每月350G流量，限制5个设备，限速300M；可用高级版节点\n",
    "旗舰版：49.9元/月，每月600G流量，限制8个设备，不限速；可用专线节点\n",
    "旗舰版大容量：95.9元/月，每月1200G流量，限制10个设备，不限速；可用专线节点\n",
    "其他情况：\n",
    "\n",
    "支持SS，Vmess\n",
    "包年9折优惠代码：lanfan\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3ICa6mj\n",
    "\n",
    "\n",
    "（2025.11.21 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 蓝帆云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "42.疾风云（中转机场）\n",
    "前段时间发现的一家机场，主要是移动线路和专线，有部分冷门线路。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "入门版：9.99元/月，每月50G流量，限制1个设备，限速60M；可用入门版节点\n",
    "基础版：15.9元/月，每月100G流量，限制2个设备，限速60M；可用基础版节点\n",
    "标准版：25.9元/月，每月200G流量，限制3个设备，限速150M；可用标准版节点\n",
    "高级版：35.9元/月，每月350G流量，限制5个设备，限速300M；可用高级版节点\n",
    "旗舰版：49.9元/月，每月600G流量，限制8个设备，不限速；可用专线节点\n",
    "旗舰版大容量：95.9元/月，每月1200G流量，限制10个设备，不限速；可用专线节点\n",
    "其他情况：\n",
    "\n",
    "支持Vmess\n",
    "年费9折优惠码： jifeng\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/42G6z0R\n",
    "\n",
    "\n",
    "（2025.11.22 晚测速）\n",
    "\n",
    "更多测速详见：机场测速观察 --- 疾风云\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "43.尔湾云（中转机场）\n",
    "新发现的一家机场，线路比较齐全，速度还不错，主要是移动线路和专线，其他的后续再观察。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "入门版：11.9元/月，50G流量；35.9元/季，150G流量；71.9元/半年，300G流量；143元/年，600G流量；2个设备\n",
    "基础版：16.9元/月，100G流量；50.9元/季，300G流量；101.9元/半年，600G流量；203元/年，1200G流量；2个设备\n",
    "标准版：26.9元/月，200G流量；80.9元/季，600G流量；161.9元/半年，1200G流量；323元/年，2400G流量；3个设备\n",
    "高级版：36.9元/月，350G流量；110.9元/季，1050G流量；221.9元/半年，2100G流量；443元/年，4200G流量；5个设备\n",
    "旗舰版：59.9元/月，600G流量；179.9元/季，1800G流量；359.9元/半年，3600G流量；719元/年，7200G流量；8个设备\n",
    "旗舰版Max：99.9元/月，1200G流量；299.9元/季，3600G流量；599.9元/半年，7200G流量；1199元/年，14400G流量；10个设备\n",
    "其他情况：\n",
    "\n",
    "支持SS，Vmess\n",
    "年费折扣优惠码:erfan\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/45xmK1i\n",
    "\n",
    "\n",
    "（2025.11.21 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "44.Flyint飞数\n",
    "拿在手里一年多的一家机场了，多次测速还都可以就放上来了。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "1号套餐：28元/月，每月150G流量，84元/季，168元/半年，319元/年；限制7台设备\n",
    "2号套餐：45元/月，每月255G流量，135元/季，270元/半年，513元/年；限制7台设备\n",
    "3号套餐：60元/月，每月365G流量，180元/季，360元/半年，685元/年；限制20台设备\n",
    "4号套餐：86元/月，每月530G流量，258元/季，516元/半年，980元/年；限制20台设备\n",
    "5号套餐：126元/月，每月820G流量，378元/季，756元/半年，1436元/年；限制20台设备\n",
    "6号套餐：150元/月，每月1200G流量，450元/季，855元/半年，1620元/年；限制25台设备\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "支付方式：支付宝+微信\n",
    "官网：https://bit.ly/3NzPkXA\n",
    "\n",
    "\n",
    "（2025.11.22 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "45.Foo&Friends（中转机场）\n",
    "已经测试过一两年的一家机场，速度还可以，主要是联通和移动中转，40 左右线路。\n",
    "\n",
    "稳定性待观察。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "LV2自行车：8.8元/月，每月60G流量；25.8元/季，50.8元/半年；限制3个客户端\n",
    "LV3全解锁：18.8元/月，每月180G流量；52.8元/季，108.8元/半年；限制5个客户端\n",
    "LV3全解锁+：45.8元/月，每月480G流量；128.8元/季，256.8元/半年；不限制客户端\n",
    "其他情况：\n",
    "\n",
    "支持SSR和V2Ray\n",
    "支付方式：支付宝+微信支付\n",
    "官网：https://bit.ly/3tn0Tuc\n",
    "\n",
    "\n",
    "（2025.11.22 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "46.守候网络\n",
    "守候网络（中转机场）\n",
    "几月前朋友介绍我的这家机场，测了几个月还不错，就放上来了。包含广州入口，江苏入口，上海入口，北京入口，深港 IPLC, 沪日 IPLC 以及优化中转线路。性价比比较高，不过新加坡、台湾节点其实香港节点，虽然解锁的是对应地区的流媒体。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "轻量套餐：20元/月，每月200G流量，会员等级2，设备限制4台，端口速率500M\n",
    "豪华版套餐：35元/月，每月500G流量，会员等级3，设备限制6台，端口速率1000M\n",
    "二零二四套餐：159元/年，总共1024G流量，会员等级3，设备限制5台，端口速率1000M\n",
    "Premium高级套餐：688元/年，每月1000G流量，会员等级10，设备不限制，端口速率1500M\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "入口：深圳联通、南京移动\n",
    "落地：香港 Hytron、Hala（自有AS）；日本 Akari；新加坡SpeedyPage；台湾 Akile；韩国Oracle；荷兰 DigitalOcean；德国 Netcup；美国 Sharktech等\n",
    "会员赠送守候影院\n",
    "支付方式：支付宝，微信，USDT\n",
    "官网：https://bit.ly/446RIfY\n",
    "\n",
    "\n",
    "（2025.11.27 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "ASH（中转机场）\n",
    "守候旗下另一家小机场，测了也几个月了还可以，20 + 节点，全 ss 中转隧道。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Mini套餐：18元/季，每月60G流量，隧道中转\n",
    "Small套餐：10元/月，每月100G流量，专线中转+隧道中转\n",
    "Medium套餐：20元/月，每月200G流量，专线中转+隧道中转\n",
    "Large套餐：30元/月，每月300G流量，专线中转+隧道中转\n",
    "Plus套餐：48元/月，每月500G流量，专线中转+隧道中转\n",
    "Basic-150G不限时：50元，150G流量，专线中转+隧道中转\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "支付方式：支付宝，微信，USDT\n",
    "官网：https://bit.ly/48rH7Nb\n",
    "\n",
    "\n",
    "（2025.11.27 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "47.MESL（中转机场）\n",
    "半年前开的一家机场，线路类型还比较丰富，流媒体解锁不错，阿里电信移动等入口。\n",
    "\n",
    "稳定性不错。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Standard TV 100G：14元/月，每月100G流量，39元/季，145元/年；允许同时在线5个公网IP\n",
    "Premium TV 100G：20元/月，每月100G流量，56元/季，198元/年；允许同时在线5个公网IP；增加阿里云专线\n",
    "Standard TV 200G：26元/月，每月200G流量，75元/季，275元/年；允许同时在线5个公网IP\n",
    "Premium TV 200G：39元/月，每月200G流量，109元/季，380元/年；允许同时在线5个公网IP；增加阿里云专线\n",
    "Standard TV 300G：36元/月，每月300G流量，99元/季，365元/年；允许同时在线5个公网IP\n",
    "Premium TV 300G：56元/月，每月300G流量，158元/季，588元/年；允许同时在线5个公网IP；增加阿里云专线\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "支付方式：支付宝+USDT\n",
    "官网：https://bit.ly/3Nt2RzV\n",
    "\n",
    "\n",
    "（2025.03.14 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "48.Bocchi（中转机场）\n",
    "也是几年前一家机场主后来开的机场，测了半年了，还算比较稳定就拿上来了。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "Starter：10元/月，每月100G流量，28元/季，102元/年\n",
    "Middle：30元/月，每月350G流量，86元/季，306元/年\n",
    "Advanced：50元/月，每月600G流量，142元/季，510元/年\n",
    "Ultimate：80元/月，每月1024G流量，228元/季，826元/年\n",
    "其他情况：\n",
    "\n",
    "支持SS\n",
    "不限制使用设备数\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3RMJLHB\n",
    "\n",
    "\n",
    "（2025.11.27 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "49.天航（中转机场）\n",
    "测试好几年的一家机场，速度还不错，一直比较稳定，就放上来了。\n",
    "\n",
    "稳定性还可以。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "经济舱VIP1：9.99元/月，每月50G流量，29.99元/季，59.8元/半年；2个在线客户端，限速80M；可用节点 ≤ VIP1\n",
    "商务舱VIP2：14元/月，每月200G流量，42元/季，168元/年；4个在线客户端，限速100M；可用节点 ≤ VIP2\n",
    "头等舱VIP3：20元/月，每月300G流量，60元/季，250元/年；5个在线客户端，限速200M；增加隧道\n",
    "旗舰版VIP4：30元/月，每月400G流量，90元/季，360元/年；8个在线客户端，限速500M；增加专线节点\n",
    "其他情况：\n",
    "\n",
    "支持V2Ray\n",
    "支付方式：支付宝+微信支付\n",
    "官网：https://bit.ly/3v6ro7O\n",
    "\n",
    "\n",
    "（2025.11.27 晚测速）\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "50.STC-Clubs（小中转机场）\n",
    "STC 家新开的一家机场，主要是比较便宜，有联通华为 AZ 等中转，试了下速度还行。分站现在已经由其他人运营。\n",
    "\n",
    "稳定性待观察。\n",
    "\n",
    "套餐情况：\n",
    "\n",
    "VIP2套餐：16元/月，150G流量，限制3个客户端；81元/半年，每月250G流量，限制3个客户端；153元/年，每月300G流量，限制4个客户端；\n",
    "VIP1套餐：8元/月，150G流量，限制2个客户端；40元/半年，每月250G流量，限制2个客户端；76元/年，每月300G流量，限制3个客户端；无国内中转。\n",
    "其他情况：\n",
    "\n",
    "支持SSR\n",
    "支付方式：支付宝\n",
    "官网：https://bit.ly/3Tr5Vk7\n",
    "\"\"\"},\n",
    "            {\"type\": \"text\", \"text\": \"请列出这个里面所有价钱小于等于 10 元 / 月，每月流量大于等于 60G 的服务商。\"},\n",
    "        ]\n",
    "    },\n",
    "]\n",
    "\n",
    "inputs = processor.apply_chat_template(\n",
    "    messages,\n",
    "    add_generation_prompt=True,\n",
    "    tokenize=True,\n",
    "    return_dict=True,\n",
    "    return_tensors=\"pt\",\n",
    ").to(model.device, dtype=torch.bfloat16)\n",
    "\n",
    "generated_ids = model.generate(**inputs, do_sample=False, max_new_tokens=1024)\n",
    "generated_texts = processor.batch_decode(\n",
    "    generated_ids,\n",
    "    skip_special_tokens=True,\n",
    ")\n",
    "print(generated_texts[0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
